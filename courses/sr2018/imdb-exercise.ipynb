{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB movie review sentiment classification with CNNs\n",
    "\n",
    "In this notebook, we'll train a convolutional neural network (CNN, ConvNet) for sentiment classification using Keras.  Keras version $\\ge$ 2 is required.  This notebook is largely based on the [`imdb_cnn.py` script](https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py) in the Keras examples.\n",
    "\n",
    "First, the needed imports. Keras tells us which backend (Theano, Tensorflow, CNTK) it will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from distutils.version import LooseVersion as LV\n",
    "from keras import __version__\n",
    "from keras import backend as K\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
    "assert(LV(__version__) >= LV(\"2.0.0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB data set\n",
    "\n",
    "Next we'll load the IMDB data set. First time we may have to download the data, which can take a while.\n",
    "\n",
    "The dataset contains 50000 movies reviews from the Internet Movie Database, split into 25000 reviews for training and 25000 reviews for testing. Half of the reviews are positive (1) and half are negative (0).\n",
    "\n",
    "The dataset has already been preprocessed, and each word has been replaced by an integer index.\n",
    "The reviews are thus represented as varying-length sequences of integers.\n",
    "(Word indices begin at \"3\", as \"1\" is used to mark the start of a review and \"2\" represents all out-of-vocabulary words. \"0\" will be used later to pad shorter reviews to a fixed size.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of most-frequent words to use\n",
    "nb_words = 10000\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=nb_words)\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "print('IMDB data loaded:')\n",
    "print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape, 'positive:', np.sum(y_train))\n",
    "print('x_test:', x_test.shape)\n",
    "print('y_test:', y_test.shape, 'positive:', np.sum(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first movie review in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First review in the training set:\\n\", x_train[0], \"length:\", len(x_train[0]), \"class:\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we can convert the review back to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in x_train[0]])\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data consists of lists of word indices of varying length.  Let's inspect the distribution of the length of the training movie reviews: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(len(x_train)):\n",
    "    l.append(len(x_train[i]))\n",
    "plt.figure()\n",
    "plt.title('Length of training reviews')\n",
    "plt.hist(l,100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's truncate the reviews to `maxlen` first words, and pad any shorter reviews with zeros at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 400\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen,\n",
    "                                 padding='post', truncating='post')\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen, \n",
    "                                padding='post', truncating='post')\n",
    "print('x_train:', x_train.shape)\n",
    "print('x_test:', x_test.shape)\n",
    "\n",
    "print(\"First review in the training set:\\n\", x_train[0], 'length:', len(x_train[0]))\n",
    "\n",
    "l = []\n",
    "for i in range(len(x_train)):\n",
    "    l.append(len(x_train[i]))\n",
    "plt.figure()\n",
    "plt.title('Length of training reviews')\n",
    "plt.hist(l,100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Let's create a 1D CNN model that has one (or optionally two) convolutional layers with *relu* as the activation function, followed by a *Dense* layer.  The first layer in the network is an *Embedding* layer that converts integer indices to dense vectors of length `embedding_dims`.  Dropout is applied after embedding and dense layers, and max pooling after the convolutional layers. The output layer contains a single neuron and *sigmoid* non-linearity to match the binary groundtruth (`y_train`). \n",
    "\n",
    "Finally, we `compile()` the model, using *binary crossentropy* as the loss function and [*RMSprop*](https://keras.io/optimizers/#rmsprop) as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters:\n",
    "embedding_dims = 50\n",
    "cnn_filters = 100\n",
    "cnn_kernel_size = 5\n",
    "dense_hidden_dims = 200\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(nb_words,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "## uncomment these if you want to use two convolutional layers:\n",
    "#model.add(Conv1D(cnn_filters, cnn_kernel_size,\n",
    "#                 padding='valid', activation='relu'))\n",
    "#model.add(MaxPooling1D(5))\n",
    "\n",
    "model.add(Conv1D(cnn_filters, cnn_kernel_size,\n",
    "                 padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(dense_hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Now we are ready to train our model.  An *epoch* means one pass through the whole training data. Note also that we are using a fraction of the training data as our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 5\n",
    "validation_split = 0.2\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=128,\n",
    "          epochs=epochs, \n",
    "          validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data to see how the training progressed. A big gap between training and validation accuracies would suggest overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'], label='training')\n",
    "plt.plot(history.epoch,history.history['val_loss'], label='validation')\n",
    "plt.title('loss')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['acc'], label='training')\n",
    "plt.plot(history.epoch,history.history['val_acc'], label='validation')\n",
    "plt.title('accuracy')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "For a better measure of the quality of the model, let's see the model accuracy for the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the learned model to predict sentiments for new reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myreviewtext = 'this movie was the worst i have ever seen and the actors were horrible'\n",
    "#myreviewtext = 'this movie is great and i madly love the plot from beginning to end'\n",
    "\n",
    "myreview = np.zeros((1,maxlen), dtype=int)\n",
    "myreview[0, 0] = 1\n",
    "\n",
    "for i, w in enumerate(myreviewtext.split()):\n",
    "    if w in word_index and word_index[w]+3<nb_words:\n",
    "        myreview[0, i+1] = word_index[w]+3\n",
    "    else:\n",
    "        print('word not in vocabulary:', w)\n",
    "        myreview[0, i+1] = 2\n",
    "\n",
    "print(myreview, \"shape:\", myreview.shape)\n",
    "\n",
    "p = model.predict(myreview, batch_size=1) # values close to \"0\" mean negative, close to \"1\" positive\n",
    "print('Predicted sentiment: {:.10f}'.format(p[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model tuning\n",
    "\n",
    "Modify the model.  Try to improve the classification accuracy on the test set, or experiment with the effects of different parameters.  \n",
    "\n",
    "You can also consult the Keras documentation at https://keras.io/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
