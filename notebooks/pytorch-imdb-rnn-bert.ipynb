{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB movie review sentiment classification with RNNs\n",
    "\n",
    "In this notebook, we'll train a recurrent neural network (RNN) for sentiment classification using **PyTorch**.\n",
    "\n",
    "First, the needed imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.4.1+cu118\n",
      "Using GPU, device name: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import models, trainers, pre_tokenizers, normalizers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU, device name:', torch.cuda.get_device_name(0))\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU found, using CPU instead.') \n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB data set\n",
    "\n",
    "Next we'll load the IMDB data set using the [Datasets library from Hugging Face](https://huggingface.co/docs/datasets/index).\n",
    "\n",
    "The dataset contains 50000 movies reviews from the Internet Movie Database, split into 25000 reviews for training and 25000 reviews for testing. Half of the reviews are positive and half are negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dir = ./data\n"
     ]
    }
   ],
   "source": [
    "slurm_project = os.getenv('SLURM_JOB_ACCOUNT')\n",
    "data_dir = os.path.join('/scratch', slurm_project, 'data') if slurm_project else './data'\n",
    "print('data_dir =', data_dir)\n",
    "\n",
    "train_dataset = load_dataset(\"imdb\", split=\"train\", trust_remote_code=False, cache_dir=data_dir)\n",
    "test_dataset = load_dataset(\"imdb\", split=\"test\", trust_remote_code=False, cache_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data items can be accessed by index, and each item is a dictionary with a 'text' and 'label' field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\",\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick count of the labels in the dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "label: 0, count: 12500\n",
      "label: 1, count: 12500\n",
      "test\n",
      "label: 0, count: 12500\n",
      "label: 1, count: 12500\n"
     ]
    }
   ],
   "source": [
    "def count_labels(dataset):\n",
    "    counts={}\n",
    "    i=0\n",
    "    for item in dataset:\n",
    "        label = item['label']\n",
    "        if label not in counts:\n",
    "            counts[label] = 1\n",
    "        else:\n",
    "            counts[label] += 1\n",
    "    for key, value in counts.items():\n",
    "        print(f\"label: {key}, count: {value}\")\n",
    "\n",
    "print('train')\n",
    "count_labels(train_dataset)\n",
    "\n",
    "print('test')\n",
    "count_labels(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have two labels: `0` and `1`, each with 12500 items per dataset split. Label `0` indicates a negative review, and label `1` a positive one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "modelname = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "#modelname = 'AdamCodd/tinybert-sentiment-amazon'\n",
    "bertmodel = pipeline(model=modelname, truncation=True, max_length=512, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x7f9f34e53f10>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.999869704246521},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997679591178894}]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertmodel([\"This was a great movie!\", \"This was the worst movie I have ever seen.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                         drop_last=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for item in test_loader:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(output, target):\n",
    "    sentiment_pred = torch.Tensor([0 if x['label']=='NEGATIVE' else 1 for x in output]).int()\n",
    "    #print(sentiment_pred)\n",
    "    #print(target.int())\n",
    "    correct_ones = sentiment_pred == target.int()  # 1 for correct, 0 for incorrect\n",
    "    #print(correct_ones)\n",
    "    return correct_ones.sum().item()               # count number of correct ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model):\n",
    "    \n",
    "    num_items = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for item in test_loader:\n",
    "            # Copy data and targets to GPU\n",
    "            #data = KeyDataset(item, \"text\")\n",
    "            data = item['text']\n",
    "            target = item['label'] #.to(device)\n",
    "\n",
    "            #print(data)\n",
    "            # Do a forward pass\n",
    "            output = model(data)\n",
    "            #print(output, target)\n",
    "            #for a in enumerate(output):\n",
    "            #    print(a)\n",
    "            #print(model.postprocess(output))\n",
    "            # Count number of correct digits\n",
    "            total_correct += correct(output, target)\n",
    "            num_items += len(target)\n",
    "            #if num_items>10:\n",
    "            #    break\n",
    "            \n",
    "    accuracy = total_correct/num_items\n",
    "\n",
    "    print(f\"Testset accuracy: {100*accuracy:>0.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset accuracy: 89.1%\n",
      "CPU times: user 4min 26s, sys: 414 ms, total: 4min 26s\n",
      "Wall time: 4min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "test(test_loader, bertmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "Before we start training, we need to process the data into a more suitable format. The text now consists of text strings of variable length, but a neural network typically needs to have fixed-length vectors containing numbers.\n",
    "\n",
    "To achieve this we will use the `WordLevel` tokenizer from Hugging Face's [Tokenizers library](https://huggingface.co/docs/tokenizers/index). We will tell it to create a vocabulary of the 10,000 most frequent words, and use the special word `[UNK]` for any other words. These 10,001 words will all be mapped to a specific integer index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of most-frequent words to use\n",
    "nb_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(models.WordLevel(unk_token='[UNK]'))\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer.normalizer = normalizers.Sequence([normalizers.NFD(),\n",
    "                                             normalizers.Lowercase(),\n",
    "                                             normalizers.StripAccents()])\n",
    "\n",
    "trainer = trainers.WordLevelTrainer(vocab_size=nb_words, min_frequency=1, special_tokens=['[UNK]'])\n",
    "tokenizer.train_from_iterator(train_dataset['text'], trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our tokenizer out with an example sentence. We deliberately also add a nonsense word to see if it correctly maps that to `[UNK]` (which has index 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4793, 2, 14, 9, 5, 2246, 4252, 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"hello, this is a test sentence foobazz\").ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create a function that ensures all our vector have the same length of 80 by truncating too long sentences and padding too short ones with 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_length = 80\n",
    "\n",
    "def text_transform(text):\n",
    "    x = tokenizer.encode(text)\n",
    "    x.truncate(vec_length)\n",
    "    x.pad(vec_length)\n",
    "    return x.ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try the text transformation on a test sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4793, 2, 14, 9, 5, 2246, 4252, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(text_transform(\"hello, this is a test sentence\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's apply this by transforming our datasets to represent our texts as 80-length vectors and labels as floating point values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265e32e8a4f540cea6e15061eba94043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99bfe077d014b12b20d238444b58a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the list of transforms to the text\n",
    "# We also switch around so we have the text first and labels second\n",
    "def apply_transform(x):\n",
    "    return {\n",
    "        'input_ids': text_transform(x['text']),\n",
    "        'label_id': float(x['label'])\n",
    "        }\n",
    "\n",
    "train_dataset_tr = train_dataset.map(apply_transform, remove_columns=['text', 'label']).with_format('torch')\n",
    "test_dataset_tr = test_dataset.map(apply_transform, remove_columns=['text', 'label']).with_format('torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 105,    5,  511,  877,   27,   24,    5,  191, 2458,   17, 2727,  224,\n",
       "          872,   15,  108,    1,  128,  332,    5,  132,  145,    4, 3111,    1,\n",
       "          527,    6,    1,  377, 1203,    0,  122,    3,  555,    6,    1,  128,\n",
       "          102,    8,   29,  108,  307,    4,  332,    7,   38, 1321,   12,    7,\n",
       "          384,   66,   67,    3,    1,  148,  128,    9,   15,    6,    5,   65,\n",
       "         1153,  576,   15,  170,    8,   29,  585,    1,  527, 3197,    4,  114,\n",
       "         1336,   35,  603,   47,    1,  479,    3,    1]),\n",
       " 'label_id': tensor(0.)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_tr[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create the data loaders with a given batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset_tr, batch_size=batch_size, shuffle=True,\n",
    "                          drop_last=True)\n",
    "test_loader = DataLoader(dataset=test_dataset_tr, batch_size=batch_size, shuffle=False,\n",
    "                         drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model\n",
    "\n",
    "Let's create an RNN model that contains an LSTM layer. The first layer in the network is an *embedding* layer that converts integer indices to dense vectors of length `embedding_dims`. The output layer contains a single neuron and *sigmoid* non-linearity to match the binary groundtruth (0=negative, 1=positive review). \n",
    "\n",
    "All the [neural network building blocks defined in PyTorch can be found in the torch.nn documentation](https://pytorch.org/docs/stable/nn.html).\n",
    "\n",
    "The output of [LSTM in PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) is a 3D tensor of the shape batch_size x sequence_length x lstm_units, that is we get the output after each item in the sequence. Here we only want to have the output after the last item (after the whole sentence has been processed). This means we have to do things a bit more manually and cannot use the simple `nn.Sequential` as in previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters:\n",
    "embedding_dims = 50\n",
    "lstm_units = 32\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(nb_words, embedding_dims)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.lstm = nn.LSTM(embedding_dims, lstm_units, batch_first=True)\n",
    "        self.linear = nn.Linear(lstm_units, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.dropout(x)\n",
    "        x, (hn, cn) = self.lstm(x)    # LSTM also returns the values of the internal h_n and c_n parameters\n",
    "        x = self.linear(x[:, -1, :])  # we pick only the last output after having processed the whole sequence\n",
    "        return self.sigmoid(x.view(-1))\n",
    "\n",
    "model = SimpleRNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Now let's train the RNN model. Note that LSTMs are rather slow to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(output, target):\n",
    "    sentiment_pred = output.round().int()          # set to 0 for <0.5 and 1 for >0.5\n",
    "    correct_ones = sentiment_pred == target.int()  # 1 for correct, 0 for incorrect\n",
    "    return correct_ones.sum().item()               # count number of correct ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    num_batches = 0\n",
    "    num_items = 0\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for item in tqdm(data_loader):\n",
    "        # Copy data and targets to GPU\n",
    "        data = item['input_ids'].to(device)\n",
    "        target = item['label_id'].to(device)\n",
    "        \n",
    "        # Do a forward pass\n",
    "        output = model(data)\n",
    "      \n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Count number of correct digits\n",
    "        total_correct += correct(output, target)\n",
    "        num_items += len(target)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    train_loss = total_loss/num_batches\n",
    "    accuracy = total_correct/num_items\n",
    "    print(f\"Average loss: {train_loss:7f}, accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [binary cross-entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) and [RMSprop optimizer](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Training epoch: {epoch+1}\")\n",
    "    train(train_loader, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Here we have the same `test` function as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    num_batches = 0\n",
    "    num_items = 0\n",
    "\n",
    "    test_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for item in test_loader:\n",
    "            # Copy data and targets to GPU\n",
    "            data = item['input_ids'].to(device)\n",
    "            target = item['label_id'].to(device)\n",
    "\n",
    "            # Do a forward pass\n",
    "            output = model(data)\n",
    "        \n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "            # Count number of correct digits\n",
    "            total_correct += correct(output, target)\n",
    "            num_items += len(target)\n",
    "\n",
    "    test_loss = test_loss/num_batches\n",
    "    accuracy = total_correct/num_items\n",
    "\n",
    "    print(f\"Testset accuracy: {100*accuracy:>0.1f}%, average loss: {test_loss:>7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the learned model to predict sentiments for new reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myreviewtext = 'this movie was the worst i have ever seen and the actors were horrible'\n",
    "myreviewtext = 'this movie was awesome and then best action I have ever seen'\n",
    "\n",
    "input = torch.tensor(text_transform(myreviewtext)).view(1, -1).to(device)\n",
    "print(input)\n",
    "p = model(input).item()\n",
    "sentiment = \"POSITIVE\" if p > 0.5 else \"NEGATIVE\"\n",
    "print(f'Predicted sentiment: {sentiment} ({p:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Two LSTM layers\n",
    "\n",
    "Create a model with two LSTM layers (hint: there is a `num_layers` option!). Optionally, you can also use bidirectional layers (set `bidirectional=True` in LSTM). See the [LSTM documentation in PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM).\n",
    "\n",
    "You can consult the [PyTorch documentation](https://pytorch.org/docs/stable/index.html), in particular all the [neural network building blocks can be found in the `torch.nn` documentation](https://pytorch.org/docs/stable/nn.html).\n",
    "\n",
    "The code below is missing the model definition. You can copy any suitable layers from the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayeredRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TASK 1: ADD LAYERS HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute cell to see the example answer.\n",
    "\n",
    "**Note:** in Google Colab you have to click and copy the answer manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/pytorch-imdb-rnn-example-answer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_model = TwoLayeredRNN()\n",
    "print(ex1_model)\n",
    "\n",
    "assert len(list(ex1_model.parameters())) > 0, \"ERROR: You need to write the missing model definition above!\"\n",
    "\n",
    "\n",
    "ex1_model = ex1_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_criterion = nn.BCELoss()\n",
    "ex1_optimizer = torch.optim.RMSprop(ex1_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch+1} ...\")\n",
    "    train(train_loader, ex1_model, ex1_criterion, ex1_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader, ex1_model, ex1_criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Model tuning\n",
    "\n",
    "Modify the model further.  Try to improve the classification accuracy on the test set, or experiment with the effects of different parameters.\n",
    "\n",
    "To combat overfitting, you can try for example to add dropout. For [LSTMs](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM), dropout between the LSTM layers can be set with the `dropout` parameter:\n",
    "\n",
    "    self.lstm = nn.LSTM(embedding_dims, lstm_units, num_layers=2,\n",
    "                        batch_first=True, dropout=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to change the batch size, you need to re-define the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
