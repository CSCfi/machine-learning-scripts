{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California housing dataset regression with MLPs\n",
    "\n",
    "In this notebook, we'll train a multi-layer perceptron model to to estimate median house values on Californian housing districts using [TensorFlow](https://www.tensorflow.org/) (version $\\ge$ 2.0 required) with the [Keras API](https://www.tensorflow.org/guide/keras/overview).\n",
    "\n",
    "First, the needed imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "\n",
    "from distutils.version import LooseVersion as LV\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "print('Using Tensorflow version: {}, and Keras version: {}.'.format(tf.__version__, tf.keras.__version__))\n",
    "assert(LV(tf.__version__) >= LV(\"2.0.0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Then we load the California housing data. First time we need to download the data, which can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chd = datasets.fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of 20640 housing districts, each characterized with 8 attributes: *MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude*. There is also a target value *MedHouseVal* (median house value) for each housing district.\n",
    " \n",
    "Let's plot all attributes against the target value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "for i in range(8):\n",
    "    plt.subplot(4,2,i+1)\n",
    "    plt.scatter(chd.data[:,i], chd.target, s=2, label=chd.feature_names[i])\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylabel('MedHouseVal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll split the data into a training and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 5000\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    chd.data, chd.target, test_size=test_size, shuffle=True)\n",
    "\n",
    "print()\n",
    "print('California housing data: train:',len(X_train),'test:',len(X_test))\n",
    "print()\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print()\n",
    "print('X_test', X_test.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data matrix `X_train` is a matrix of size (20640-`test_size`, 8), and the vector `y_train` contains the target value (median house value) for each housing district in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hidden layer\n",
    "\n",
    "### Initialization\n",
    "\n",
    "Let's begin with a simple model that has a single hidden layer.  \n",
    "\n",
    "We first create the `Input` with `X_train.shape[1]` inputs (one for each of the eight attributes in the training data).\n",
    "\n",
    "As the first thing, we scale the input data feature-wise to zero mean and unit variance. This is done using a special `Normalization` layer, whose parameters are not set during training the network. Instead, we compute the mean and variance of the training data and store them as the layer's weights with separate call to the `adapt()` function. \n",
    "\n",
    "Next we add a `Dense` layer that has 10 output nodes. The `Dense` layer connects each input to each output with some weight parameter and then passes the result through a ReLU non-linear activation function.\n",
    "\n",
    "Then we have the output layer that has only one unit with a linear activation function.\n",
    "\n",
    "After all layers are created, we create the `Model` by specifying its inputs and outputs.\n",
    "\n",
    "Finally, we select *mean squared error* as the loss function, select [*stochastic gradient descent*](https://keras.io/optimizers/#sgd) as the optimizer, and `compile()` the model. Note there are [several different options](https://keras.io/optimizers/) for the optimizer in Keras that we could use instead of *sgd*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slinputs = keras.Input(shape=(X_train.shape[1],))\n",
    "\n",
    "normlayer = layers.experimental.preprocessing.Normalization()\n",
    "normlayer.adapt(X_train)\n",
    "x = normlayer(slinputs)\n",
    "\n",
    "x = layers.Dense(units=10, activation=\"relu\")(x)\n",
    "sloutputs = layers.Dense(units=1, activation='linear')(x)\n",
    "\n",
    "slmodel = keras.Model(inputs=slinputs, outputs=sloutputs,\n",
    "                    name=\"single_layer_mlp_model\")\n",
    "slmodel.compile(loss='mean_squared_error', \n",
    "                optimizer='sgd',\n",
    "                metrics=[keras.metrics.MeanAbsoluteError()])\n",
    "print(slmodel.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also draw a fancier graph of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(slmodel, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "Now we are ready to train our first model.  An *epoch* means one pass through the whole training data. \n",
    "\n",
    "You can run code below multiple times and it will continue the training process from where it left off.  If you want to start from scratch, re-initialize the model using the code a few cells ago. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 10 \n",
    "\n",
    "slhistory = slmodel.fit(X_train, \n",
    "                        y_train, \n",
    "                        epochs=epochs, \n",
    "                        batch_size=32,\n",
    "                        verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how the training progressed. *Loss* is a function of the difference of the network output and the target values.  We are minimizing the loss function during training so it should decrease over time. We also monitor *mean absolute error* during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,3))\n",
    "ax1.plot(slhistory.epoch,slhistory.history['loss'])\n",
    "ax1.set_title('loss (mean squared error)')\n",
    "ax1.set_xlabel('epoch');\n",
    "ax2.plot(slhistory.epoch,slhistory.history['mean_absolute_error'])\n",
    "ax2.set_title('mean absolute error')\n",
    "ax2.set_xlabel('epoch');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "For a better measure of the quality of the model, let's see the mean squared error for the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "slpred = slmodel.predict(X_test)\n",
    "print(\"Mean squared error: %.3f\"\n",
    "      % mean_squared_error(y_test, slpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the individual errors made by the model when making the predictions. The true values of the target variable (*MedHouseVal*) are on the horizontal axis and the corresponding predictions are on the vertical axis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.axes(aspect='equal')\n",
    "plt.scatter(y_test, slpred, s=1)\n",
    "plt.xlabel('true values')\n",
    "plt.ylabel('predictions')\n",
    "lims = [-0.1, 5.1]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "plt.plot(lims, lims, color=sns.color_palette()[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Let's now create a more complex MLP model that has multiple dense layers and dropout layers.  `Dropout()` randomly sets a fraction of inputs to zero during training, which is one approach to regularization and can sometimes help to prevent overfitting.\n",
    "\n",
    "The output layer needs again to have a single unit with linear activation. \n",
    "\n",
    "Finally, we again `compile()` the model, this time using [*Adam*](https://keras.io/optimizers/#adam) as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlinputs = keras.Input(shape=(X_train.shape[1],))\n",
    "\n",
    "normlayer = layers.experimental.preprocessing.Normalization()\n",
    "normlayer.adapt(X_train)\n",
    "x = normlayer(mlinputs)\n",
    "\n",
    "x = layers.Dense(units=20, activation=\"relu\")(x)\n",
    "x = layers.Dense(units=20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "mloutputs = layers.Dense(units=1, activation='linear')(x)\n",
    "\n",
    "mlmodel = keras.Model(inputs=mlinputs, outputs=mloutputs,\n",
    "                    name=\"multi_layer_mlp_model\")\n",
    "mlmodel.compile(loss='mean_squared_error', \n",
    "                optimizer='adam',\n",
    "                metrics=[keras.metrics.MeanAbsoluteError()])\n",
    "print(mlmodel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(mlmodel, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 10 \n",
    "\n",
    "mlhistory = mlmodel.fit(X_train, \n",
    "                        y_train, \n",
    "                        epochs=epochs, \n",
    "                        batch_size=32,\n",
    "                        verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,3))\n",
    "ax1.plot(mlhistory.epoch,mlhistory.history['loss'])\n",
    "ax1.set_title('loss (mean squared error)')\n",
    "ax1.set_xlabel('epoch');\n",
    "ax2.plot(mlhistory.epoch,mlhistory.history['mean_absolute_error'])\n",
    "ax2.set_title('mean absolute error')\n",
    "ax2.set_xlabel('epoch');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mlpred = mlmodel.predict(X_test)\n",
    "print(\"Mean squared error: %.3f\"\n",
    "      % mean_squared_error(y_test, mlpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.axes(aspect='equal')\n",
    "plt.scatter(y_test, mlpred, s=1)\n",
    "plt.xlabel('true values')\n",
    "plt.ylabel('predictions')\n",
    "lims = [-0.1, 5.1]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "plt.plot(lims, lims, color=sns.color_palette()[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning\n",
    "\n",
    "Try to reduce the mean squared error of the regression. Modify the network architectures and see if the results improve. See the documentation of [Keras](https://keras.io/) for further options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
