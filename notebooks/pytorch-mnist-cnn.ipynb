{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST handwritten digit classification with CNNs\n",
    "\n",
    "In this notebook, we'll train a convolutional neural network (CNN, ConvNet) to classify MNIST digits using **PyTorch**. \n",
    "\n",
    "First, the needed imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU, device name:', torch.cuda.get_device_name(0))\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU found, using CPU instead.') \n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=ToTensor())\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network\n",
    "\n",
    "Now we are ready to create a convolutional model. As before we use `nn.Sequential` to easily create a sequence of layers.\n",
    "\n",
    "Here we use:\n",
    "\n",
    "- [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d), which operates on 2D matrices so we input the digit images directly to the model (no need to \"flatten\" at this point),\n",
    "- [MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d) reduces the spatial dimensions, that is, makes the image smaller,\n",
    "- Finally we flatten the image to a vector and add two linear layers.\n",
    "\n",
    "All the [neural network building blocks defined in PyTorch can be found in the torch.nn documentation](https://pytorch.org/docs/stable/nn.html).\n",
    "\n",
    "The output of the last layer should be normalized with softmax, but this is actually included implicitly in the loss function in PyTorch (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding='valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*13*13, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** one slightly tricky thing in the code above is that you have to know the input dimension for the first linear layer. This is the output of the `Conv2d` followed by the `MaxPool2d`. This can be reasoned as follows:\n",
    "\n",
    "- the input to `Conv2d` will be 1x28x28 as the images have a single color channel (gray scale) and have a width by height of 28x28\n",
    "- the output of `Conv2d` will be 32x26x26 as the color channels are replaced by the outputs of the 32 convolution kernels, and due the valid padding and kernel size of 3x3 a border of 1 pixel will be excluded\n",
    "- `MaxPool2d` will remove every second pixel along each dimension, so we get 32x13x13=5408\n",
    "\n",
    "If you are lazy you can also just guess something and run the code. The error message will tell you what size it expected to have!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Now let's train the CNN model.\n",
    "\n",
    "First we'll define the same functions as in the previous exercise. In the `train` function we just added `tqdm` to print a nicer progress bar as the training will be a bit slower this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(output, target):\n",
    "    predicted_digits = output.argmax(1)                            # pick digit with largest network output\n",
    "    correct_ones = (predicted_digits == target).type(torch.float)  # 1.0 for correct, 0.0 for incorrect\n",
    "    return correct_ones.sum().item()                               # count number of correct ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    num_batches = len(data_loader)\n",
    "    num_items = len(data_loader.dataset)\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for data, target in tqdm(data_loader, total=num_batches):\n",
    "        # Copy data and targets to GPU\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # Do a forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Count number of correct digits\n",
    "        total_correct += correct(output, target)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    train_loss = total_loss/num_batches\n",
    "    accuracy = total_correct/num_items\n",
    "    print(f\"Average loss: {train_loss:7f}, accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a relatively complex model, so training is considerably slower than with MLPs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Training epoch: {epoch+1}\")\n",
    "    train(train_loader, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Here we have the same `test` function as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    num_batches = len(test_loader)\n",
    "    num_items = len(test_loader.dataset)\n",
    "\n",
    "    test_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Copy data and targets to GPU\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "        \n",
    "            # Do a forward pass\n",
    "            output = model(data)\n",
    "        \n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "        \n",
    "            # Count number of correct digits\n",
    "            total_correct += correct(output, target)\n",
    "\n",
    "    test_loss = test_loss/num_batches\n",
    "    accuracy = total_correct/num_items\n",
    "\n",
    "    print(f\"Testset accuracy: {100*accuracy:>0.1f}%, average loss: {test_loss:>7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: A more complex CNN model\n",
    "\n",
    "Your task is to try the same problem as above, but with two convolutional layers. The new model should have the following layers in order:\n",
    "\n",
    "- Convolutional (`Conv2d`) layer with 32 units and 3x3 kernels, valid padding + ReLU activation\n",
    "- Another identical convolutional layer + ReLU activation\n",
    "- Max pooling (`MaxPool2d`) layer with 2x2 pooling size\n",
    "- Dropout with 0.25 rate\n",
    "- Flatten\n",
    "- Dense layer with 128 units\n",
    "- Dropout with 0.5 rate\n",
    "- Dense output layer with 10 units\n",
    "\n",
    "You can consult the [PyTorch documentation](https://pytorch.org/docs/stable/index.html), in particular all the [neural network building blocks can be found in the `torch.nn` documentation](https://pytorch.org/docs/stable/nn.html).\n",
    "\n",
    "The code below is missing the model definition. You can copy any suitable layers from the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "           # TASK 1: ADD LAYERS HERE\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute cell to see the example answer.\n",
    "\n",
    "**Note:** in Google Colab you have to click and copy the answer manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/pytorch-mnist-cnn-example-answer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_model = ComplexCNN()\n",
    "print(ex1_model)\n",
    "\n",
    "assert len(ex1_model.layers) > 0, \"ERROR: You need to write the missing model definition above!\"\n",
    "ex1_model = ex1_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_criterion = nn.CrossEntropyLoss()\n",
    "ex1_optimizer = torch.optim.Adam(ex1_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch+1} ...\")\n",
    "    train(train_loader, ex1_model, ex1_criterion, ex1_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader, ex1_model, ex1_criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Tune training parameters\n",
    "\n",
    "Try to improve the classification accuracy, in particular by trying different optimizers and playing with the parameters of the training process.\n",
    "\n",
    "See optimizers available in PyTorch here: <https://pytorch.org/docs/stable/optim.html#algorithms>\n",
    "\n",
    "You can take the model created in Task 1 as a starting point. Below is a code example which you can modify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to change the batch size, you need to re-define the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2_model = ComplexCNN().to(device)\n",
    "\n",
    "ex2_criterion = nn.CrossEntropyLoss()\n",
    "ex2_optimizer = torch.optim.Adam(ex2_model.parameters())\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch+1} ...\")\n",
    "    train(train_loader, ex2_model, ex2_criterion, ex2_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader, ex2_model, ex2_criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Run this notebook in Google Colaboratory using [this link](https://colab.research.google.com/github/csc-training/intro-to-dl/blob/master/day1/optional/pytorch-mnist-mlp.ipynb).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
