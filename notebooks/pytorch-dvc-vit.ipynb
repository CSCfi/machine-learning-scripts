{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Dogs-vs-cats classification with CNNs\n",
    "\n",
    "In this notebook, we'll finetune a [Vision Transformer](https://arxiv.org/abs/2010.11929) (ViT) to classify images of dogs from images of cats using PyTorch and HuggingFace's [Transformers](https://github.com/huggingface/transformers). \n",
    "\n",
    "**Note that using a GPU with this notebook is highly recommended.**\n",
    "\n",
    "First, the needed imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from distutils.version import LooseVersion as LV\n",
    "\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from transformers import __version__ as transformers_version\n",
    "\n",
    "torch.manual_seed(42)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__,\n",
    "      'Transformers version:', transformers_version,\n",
    "      'Device:', device)\n",
    "assert(LV(torch.__version__) >= LV(\"1.0.0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "TensorBoard is a tool for visualizing progress during training.  Although TensorBoard was created for TensorFlow, it can also be used with PyTorch.  It is easiest to use it with the tensorboardX module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorboardX\n",
    "    import os, datetime\n",
    "    logdir = os.path.join(os.getcwd(), \"logs\",\n",
    "                          \"dvc-vit-\"+datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "    print('TensorBoard log directory:', logdir)\n",
    "    os.makedirs(logdir)\n",
    "    log = tensorboardX.SummaryWriter(logdir)\n",
    "except ImportError as e:\n",
    "    log = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "The training dataset consists of 2000 images of dogs and cats, split in half.  In addition, the validation set consists of 1000 images, and the test set of 22000 images.  Here are some random training images:\n",
    "\n",
    "![title](imgs/dvc.png)\n",
    "\n",
    "### Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_scratch = os.getenv('LOCAL_SCRATCH')\n",
    "#default_path = '/scratch/project_2005299/data'\n",
    "default_path = '/media/data/data'\n",
    "\n",
    "datapath = local_scratch if local_scratch is not None else default_path\n",
    "datapath = os.path.join(datapath, 'dogs-vs-cats/train-2000')\n",
    "\n",
    "if local_scratch is not None and not os.path.exists(datapath):\n",
    "    os.system('tar xf /scratch/project_2005299/data/dogs-vs-cats.tar -C ' + local_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data from\", datapath)\n",
    "\n",
    "(nimages_train, nimages_validation, nimages_test) = (2000, 1000, 22000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Data loaders\n",
    "\n",
    "First we specify the pre-trained ViT model we are going to use. The model [`\"google/vit-base-patch16-224\"`](https://huggingface.co/google/vit-base-patch16-224) is pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224.\n",
    "\n",
    "We'll use a pre-trained ViT feature extractor that matches the ViT model to preprocess the input images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VITMODEL = 'google/vit-base-patch16-224'\n",
    "model = ViTForImageClassification.from_pretrained(VITMODEL,\n",
    "                                                  num_labels=1,\n",
    "                                                  ignore_mismatched_sizes=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(VITMODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we define a \"collator\" function. This is just a function passed to the `DataLoader` which will pre-process each batch of data. In our case we will pass the images through the `ViTFeatureExtractor` which will process the images into the correct format for ViT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationCollator:\n",
    "    def __init__(self, feature_extractor):\n",
    "        self.feature_extractor = feature_extractor\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        x = self.feature_extractor([x[0] for x in batch], return_tensors='pt')\n",
    "        x['labels'] = torch.tensor([x[1] for x in batch], dtype=torch.float32)\n",
    "        return x\n",
    "\n",
    "collator = ImageClassificationCollator(feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's now define our data loaders for training, validation, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "\n",
    "print('Train: ', end=\"\")\n",
    "train_dataset = datasets.ImageFolder(root=datapath+'/train',\n",
    "                                     transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                          shuffle=True, num_workers=4,\n",
    "                          collate_fn=collator)\n",
    "print('Found', len(train_dataset), 'images belonging to',\n",
    "     len(train_dataset.classes), 'classes')\n",
    "\n",
    "print('Validation: ', end=\"\")\n",
    "validation_dataset = datasets.ImageFolder(root=datapath+'/validation',\n",
    "                                         transform=transforms.ToTensor())\n",
    "validation_loader = DataLoader(validation_dataset, \n",
    "                               batch_size=batch_size, shuffle=True, \n",
    "                               num_workers=4, collate_fn=collator)\n",
    "print('Found', len(validation_dataset), 'images belonging to',\n",
    "     len(validation_dataset.classes), 'classes')\n",
    "\n",
    "print('Test: ', end=\"\")\n",
    "test_dataset = datasets.ImageFolder(root=datapath+'/test',\n",
    "                                    transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                         shuffle=False, num_workers=4, \n",
    "                         collate_fn=collator)\n",
    "print('Found', len(test_dataset), 'images belonging to',\n",
    "     len(test_dataset.classes), 'classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch, scores=None):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        # Copy data to GPU if needed\n",
    "        data[\"pixel_values\"] = data[\"pixel_values\"].to(device)\n",
    "        data[\"labels\"] = data[\"labels\"].to(device)\n",
    "\n",
    "        # Zero gradient buffers\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass data through the network\n",
    "        output = model(**data)\n",
    "\n",
    "        # output is actually a SequenceClassifierOutput object that contains\n",
    "        # the loss and classification scores (logits)\n",
    "\n",
    "        # add up the loss\n",
    "        epoch_loss += output.loss.data.item()\n",
    "\n",
    "        # Backpropagate\n",
    "        output.loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss /= len(train_loader.dataset)\n",
    "    print('Train Epoch: {}, Loss: {:.4f}'.format(epoch, epoch_loss))\n",
    "\n",
    "    if scores is not None:\n",
    "        if 'loss' not in scores:\n",
    "            scores['loss'] = []\n",
    "        scores['loss'].append(epoch_loss)\n",
    "\n",
    "    if log is not None:\n",
    "        log.add_scalar('loss', epoch_loss, epoch-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(loader, scores=None, iteration=-1):\n",
    "    model.eval()\n",
    "    loss, correct = 0, 0\n",
    "    for data in loader:\n",
    "        data[\"pixel_values\"] = data[\"pixel_values\"].to(device)\n",
    "        data[\"labels\"] = data[\"labels\"].to(device)\n",
    "\n",
    "        output = model(**data)\n",
    "\n",
    "        loss += output.loss.data.item()\n",
    "\n",
    "        pred = output.logits>0.5\n",
    "        lab = data[\"labels\"].unsqueeze(1)\n",
    "        correct += pred.eq(lab).cpu().sum()\n",
    "\n",
    "    loss /= len(loader.dataset)\n",
    "\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(loader.dataset)\n",
    "\n",
    "    print('Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        loss, correct, len(loader.dataset), accuracy))\n",
    "\n",
    "    if scores is not None:\n",
    "        if 'loss' not in scores:\n",
    "            scores['loss'] = []\n",
    "        if 'accuracy' not in scores:\n",
    "            scores['accuracy'] = []\n",
    "        scores['loss'].append(loss)\n",
    "        scores['accuracy'].append(accuracy)\n",
    "\n",
    "    if log is not None and iteration >= 0:\n",
    "        log.add_scalar('val_loss', loss, iteration)\n",
    "        log.add_scalar('val_acc', accuracy, iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "train_scores = {}\n",
    "valid_scores = {}\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch, train_scores)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print('\\nValidation:')\n",
    "        evaluate(validation_loader, valid_scores, epoch-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(1,epochs+1)\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(x, train_scores['loss'], label='training')\n",
    "plt.plot(x, valid_scores['loss'], label='validation')\n",
    "plt.title('loss')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(x, valid_scores['accuracy'], label='validation')\n",
    "plt.title('accuracy')\n",
    "plt.legend(loc='best');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    evaluate(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "name": "pytorch-dvc-cnn.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
