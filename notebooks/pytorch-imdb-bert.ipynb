{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNKaJz5j_ylj"
   },
   "source": [
    "# IMDB movie review sentiment classification with BERT finetuning\n",
    "\n",
    "In this notebook, we'll use a pre-trained [BERT](https://arxiv.org/abs/1810.04805) model for text classification using PyTorch and [PyTorch-Transformers](https://github.com/huggingface/pytorch-transformers). This notebook is based on [\"Predicting Movie Review Sentiment with BERT on TF Hub\"](https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb) by Google and [\"BERT Fine-Tuning Tutorial with PyTorch\"](https://mccormickml.com/2019/07/22/BERT-fine-tuning/) by Chris McCormick.\n",
    "\n",
    "**Note that using a GPU with this notebook is highly recommended.**\n",
    "\n",
    "First, the needed imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ok002ceNB8E7",
    "outputId": "06ef90d2-7518-4209-da66-1dd45c357c78"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import (TensorDataset, DataLoader,\n",
    "                              RandomSampler, SequentialSampler)\n",
    "\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "from distutils.version import LooseVersion as LV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    devicename = '['+torch.cuda.get_device_name(0)+']'\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    devicename = \"\"\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__,\n",
    "      'Device:', device, devicename)\n",
    "assert(LV(torch.__version__) >= LV(\"1.0.0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "guw6ZNtaswKc"
   },
   "source": [
    "## IMDB data set\n",
    "\n",
    "We'll use the [raw version of the IMDB dataset](http://ai.stanford.edu/~amaas/data/sentiment/). \n",
    "For convenience, the IMDB training and test sets has in advance been converted to Pandas DataFrames.\n",
    "\n",
    "First time we may have to download the data, which can take a while.\n",
    "\n",
    "The ground truth consists of binary sentiments for each review: positive (1) or negative (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(\"/media/data/imdb/imdb-train.pkl\")\n",
    "test_df  = pd.read_pickle(\"/media/data/imdb/imdb-test.pkl\")\n",
    "\n",
    "# possibly reduce the amount of training data:\n",
    "#train_df = train_df.sample(5000)\n",
    "\n",
    "print('IMDB data loaded:')\n",
    "print('train:', train_df.shape)\n",
    "print('test:', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view some random training reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "AQfTaYDo42zu",
    "outputId": "eb8bb9de-af4c-4eb9-d075-85809e063e55"
   },
   "outputs": [],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews, i.e, our input data, are in the *sentence* column and the labels in the *polarity* column.\n",
    "\n",
    "The token `[CLS]` is a special token required by BERT at the beginning of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GuE5BqICAne2"
   },
   "outputs": [],
   "source": [
    "sentences_train = train_df.sentence.values\n",
    "sentences_train = [\"[CLS] \" + s for s in sentences_train]\n",
    "\n",
    "sentences_test = test_df.sentence.values\n",
    "sentences_test = [\"[CLS] \" + s for s in sentences_test]\n",
    "\n",
    "labels_train = train_df.polarity.values\n",
    "labels_test  = test_df.polarity.values\n",
    "\n",
    "print (\"The first training sentence:\")\n",
    "print(sentences_train[0], 'LABEL:', labels_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTREubVNFiz4"
   },
   "source": [
    "Next we specify the pre-trained BERT model we are going to use. The model `\"bert-base-uncased\"` is the lowercased \"base\" model (12-layer, 768-hidden, 12-heads, 110M parameters).\n",
    "\n",
    "We load the used vocabulary from the BERT model, and use the BERT tokenizer to convert the sentences into tokens that match the data the BERT model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "Z474sSC6oe7A",
    "outputId": "fbaa8fd8-bccd-4feb-ce52-beba5d293cfa"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "BERTMODEL='bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERTMODEL,\n",
    "                                          do_lower_case=True)\n",
    "\n",
    "tokenized_train = [tokenizer.tokenize(s) for s in sentences_train]\n",
    "tokenized_test  = [tokenizer.tokenize(s) for s in sentences_test]\n",
    "\n",
    "print (\"The full tokenized first training sentence:\")\n",
    "print (tokenized_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cp9BPRd1tMIo"
   },
   "source": [
    "Now we set the maximum sequence lengths for our training and test sentences as `MAX_LEN_TRAIN` and `MAX_LEN_TEST`. The maximum length supported by the used BERT model is 512.\n",
    "\n",
    "The token `[SEP]` is another special token required by BERT at the end of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cp9BPRd1tMIo"
   },
   "outputs": [],
   "source": [
    "MAX_LEN_TRAIN, MAX_LEN_TEST = 128, 512\n",
    "\n",
    "tokenized_train = [t[:(MAX_LEN_TRAIN-1)]+['SEP'] for t in tokenized_train]\n",
    "tokenized_test  = [t[:(MAX_LEN_TEST-1)]+['SEP'] for t in tokenized_test]\n",
    "\n",
    "print (\"The truncated tokenized first training sentence:\")\n",
    "print (tokenized_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the BERT tokenizer to convert each token into an integer index in the BERT vocabulary. We also pad any shorter sequences to `MAX_LEN_TRAIN` or `MAX_LEN_TEST` indices with trailing zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_train]\n",
    "ids_train = np.array([np.pad(i, (0, MAX_LEN_TRAIN-len(i)), \n",
    "                             mode='constant') for i in ids_train])\n",
    "\n",
    "ids_test = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_test]\n",
    "ids_test = np.array([np.pad(i, (0, MAX_LEN_TEST-len(i)), \n",
    "                            mode='constant') for i in ids_test])\n",
    "\n",
    "print (\"The indices of the first training sentence:\")\n",
    "print (ids_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KhGulL1pExCT"
   },
   "source": [
    "BERT also requires *attention masks*, with 1 for each real token in the sequences and 0 for the padding:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDoC24LeEv3N"
   },
   "outputs": [],
   "source": [
    "amasks_train, amasks_test = [], []\n",
    "\n",
    "for seq in ids_train:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  amasks_train.append(seq_mask)\n",
    "    \n",
    "for seq in ids_test:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  amasks_test.append(seq_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFbE-UHvsb7-"
   },
   "source": [
    "We use scikit-learn's `train_test_split` to use 10% of our training data as a validation set, and then convert all data into `torch.tensor`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFbE-UHvsb7-"
   },
   "outputs": [],
   "source": [
    "(train_inputs, validation_inputs, \n",
    " train_labels, validation_labels) = train_test_split(ids_train, labels_train, \n",
    "                                                     random_state=42,\n",
    "                                                     test_size=0.1)\n",
    "(train_masks, validation_masks, \n",
    " _, _) = train_test_split(amasks_train, ids_train,\n",
    "                          random_state=42, test_size=0.1)\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks  = torch.tensor(train_masks)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks  = torch.tensor(validation_masks)\n",
    "test_inputs = torch.tensor(ids_test)\n",
    "test_labels = torch.tensor(labels_test)\n",
    "test_masks  = torch.tensor(amasks_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create PyTorch *DataLoader*s for all data sets. \n",
    "\n",
    "For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GEgLpFVlo1Z-"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "print('Train: ', end=\"\")\n",
    "train_data = TensorDataset(train_inputs, train_masks,\n",
    "                           train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, \n",
    "                              batch_size=BATCH_SIZE)\n",
    "print(len(train_data), 'reviews')\n",
    "\n",
    "print('Validation: ', end=\"\")\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks,\n",
    "                                validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data,\n",
    "                                   sampler=validation_sampler,\n",
    "                                   batch_size=BATCH_SIZE)\n",
    "print(len(validation_data), 'reviews')\n",
    "\n",
    "print('Test: ', end=\"\")\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler,\n",
    "                             batch_size=BATCH_SIZE)\n",
    "print(len(test_data), 'reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gFsCTp_mporB",
    "outputId": "dd067229-1925-4b37-f517-0c14e25420d1"
   },
   "source": [
    "## BERT model initialization\n",
    "\n",
    "We now load a pretrained BERT model with a single linear classification layer added on top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gFsCTp_mporB",
    "outputId": "dd067229-1925-4b37-f517-0c14e25420d1"
   },
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(BERTMODEL, \n",
    "                                                      num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8o-VEBobKwHk"
   },
   "source": [
    "We set the remaining hyperparameters needed for fine-tuning the pretrained model:\n",
    "  * `EPOCHS`: the number of training epochs in fine-tuning (recommended values between 2 and 4)\n",
    "  * `WEIGHT_DECAY`: weight decay for the Adam optimizer\n",
    "  * `LR`: learning rate for the Adam optimizer (2e-5 to 5e-5 recommended)\n",
    "  * `WARMUP_STEPS`: number of warmup steps to (linearly) reach the set learning rate \n",
    "  \n",
    "We also need to grab the training parameters from the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxSMw0FrptiL"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "WEIGHT_DECAY = 0.01 \n",
    "LR = 2e-5\n",
    "WARMUP_STEPS =int(0.2*len(train_dataloader))\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': WEIGHT_DECAY},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LR, eps=1e-8)\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=WARMUP_STEPS,\n",
    "                                 t_total=len(train_dataloader)*EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Let's now define functions to `train()` and `evaluate()` the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "6J-FYdx6nFE_",
    "outputId": "8e388ad1-f9db-4c7b-d080-6c0a0e964610"
   },
   "outputs": [],
   "source": [
    "def train(epoch, loss_vector=None, log_interval=200):\n",
    "  # Set model to training mode\n",
    "  model.train()\n",
    "  \n",
    "  # Loop over each batch from the training set\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "    # Copy data to GPU if needed\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # Zero gradient buffers\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(b_input_ids, token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "    loss = outputs[0]\n",
    "    if loss_vector is not None:\n",
    "        loss_vector.append(loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    scheduler.step()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % log_interval == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, step * len(b_input_ids),\n",
    "                len(train_dataloader.dataset),\n",
    "                100. * step / len(train_dataloader), loss))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "6J-FYdx6nFE_",
    "outputId": "8e388ad1-f9db-4c7b-d080-6c0a0e964610"
   },
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "  model.eval()\n",
    "\n",
    "  n_correct, n_all = 0, 0\n",
    "    \n",
    "  for batch in loader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    #print(b_input_ids.shape)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None,\n",
    "                      attention_mask=b_input_mask)\n",
    "      logits = outputs[0]\n",
    "    \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    labels = b_labels.to('cpu').numpy()\n",
    "    n_correct += np.sum(predictions == labels)\n",
    "    n_all += len(labels)\n",
    "    \n",
    "  print('Accuracy: [{}/{}] {:.4f}\\n'.format(n_correct, n_all, n_correct/n_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our model using the `train()` function. After each epoch, we evaluate the model using the validation set and `evaluate()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_lossv = []\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch, train_lossv)\n",
    "    print('\\nValidation set:')\n",
    "    evaluate(validation_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-G03mmwH3aI"
   },
   "source": [
    "Let's take a look at our training loss over all batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "colab_type": "code",
    "id": "68xreA9JAmG5",
    "outputId": "6861a923-7a4e-4d03-b6fa-3038b6e2f89d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_lossv, label='original')\n",
    "plt.plot(np.convolve(train_lossv, np.ones(101), 'same') / 101, label='averaged')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkyubuJSOzg3"
   },
   "source": [
    "## Inference\n",
    "\n",
    "For a better measure of the quality of the model, let's see the model accuracy for the test reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Test set:')\n",
    "evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to ONNX ...\n",
    "#dummy_input = next(iter(validation_dataloader))[0].to(device)\n",
    "#torch.onnx.export(model, dummy_input, \"imdb-bert.onnx\", verbose=True, input_names=['input'], output_names=['output'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT Fine-Tuning Sentence Classification.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
