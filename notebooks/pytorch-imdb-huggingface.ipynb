{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2edd322b-1e0a-4fc4-919b-e808ad472cfe",
   "metadata": {},
   "source": [
    "# IMDB movie review sentiment classification using huggingface models\n",
    "\n",
    "In this notebook, we'll test pre-trained sentiment analysis models and later finetune a DistilBERT model to perform IMDB movie review sentiment classification. This notebook is adpated from [Getting Started with Sentiment Analysis using Python](https://huggingface.co/blog/sentiment-analysis-python).\n",
    "\n",
    "First, we need to install several packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df3ca3fa-f1a7-4fd6-bfda-97417a3e90d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.8)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9243fc9-da3f-470d-9e0f-9aaa9528efcd",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1227c62-d120-4908-8d35-6bf0f236be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35124c9a-1f07-416b-834d-f9c7508f682c",
   "metadata": {},
   "source": [
    "## Check if torch is using the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b124df20-1f8a-4a5e-9975-4798bfdaf0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.4.1+rocm6.1\n",
      "Using GPU, device name: AMD Instinct MI250X\n"
     ]
    }
   ],
   "source": [
    "print('Using PyTorch version:', torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU, device name:', torch.cuda.get_device_name(0))\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU found, using CPU instead.') \n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5800ebf-82bd-4cdc-9067-66ee8480d528",
   "metadata": {},
   "source": [
    "## Use Pre-trained Sentiment Analysis Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c82b0f7-62d8-4e3f-9e99-ef3ebc6522bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998656511306763},\n",
       " {'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\",device=device)\n",
    "data = [\"I love you\", \"I hate you\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a94f5-1548-46f2-a2f9-1715113e90ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- This code snippet above utilizes the **[pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines)** class to generate predictions using models from the Hub. It applies the [default sentiment analysis model](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english?text=I+like+you.+I+love+you) to evaluate the provided list of text data.\n",
    "- The analysis results are **POSITIVE** for first entry and **NEGATIVE** for the second entry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273d882-0aa9-4d86-a1fa-fd518e2c3ce0",
   "metadata": {},
   "source": [
    "### One can also use a specific sentiment analysis model by providing the name of the model, e.g., if you want a sentiment analysis model for tweets, you can specify the model id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c183b485-adc5-447d-b3b7-bb66e173c80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POS', 'score': 0.9916695356369019},\n",
       " {'label': 'NEG', 'score': 0.9806600213050842}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specific_model = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\", device = device)\n",
    "specific_model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf45bd1-3964-4d32-944b-edd0783163bb",
   "metadata": {},
   "source": [
    "## Fine-tuning DistilBERT model using IMDB dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec252bb-b0b0-48dc-8e3a-b65b72f931c1",
   "metadata": {},
   "source": [
    "- The [IMDB](https://huggingface.co/datasets/stanfordnlp/imdb) contains 50000 movies reviews from the Internet Movie Database, split into 25000 reviews for training and 25000 reviews for testing. Half of the reviews are positive and half are negative. \n",
    "\n",
    "- IMDB dataset is relatively large, so let's use 5000 samples for training and 500 for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48bb7a7d-9194-4904-bc91-bd1adb191ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = load_dataset(\"imdb\")\n",
    "small_train_dataset = imdb[\"train\"].shuffle(seed=0).select([i for i in list(range(5000))])\n",
    "small_test_dataset = imdb[\"test\"].shuffle(seed=0).select([i for i in list(range(500))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fe9183-d554-482c-8f3a-a75096e10e14",
   "metadata": {},
   "source": [
    "To preprocess our data, we will use DistilBERT tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbf1e5fc-3831-47d0-ab3f-01d6dd70482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e45582-a0c3-4a80-8a06-d4ce232a1ead",
   "metadata": {},
   "source": [
    "- Next, we will prepare the text inputs for the model for both splits of our dataset (training and test) by using the map method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ab50bd2-e54b-4e31-a162-24923b763731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "   return tokenizer(examples[\"text\"], truncation=True)\n",
    " \n",
    "tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = small_test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b9b4c0-9aea-481b-bb26-5f9bd18228c1",
   "metadata": {},
   "source": [
    "- To speed up training, let's use a data_collator to convert your training samples to PyTorch tensors and concatenate them with the correct amount of padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f10d80c5-77c3-43a6-a7d9-47d97deef882",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0455d055-7b18-47a2-b983-e7a35e46398f",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "- We will be throwing away the pretraining head of the DistilBERT model and replacing it with a classification head fine-tuned for sentiment analysis. This enables us to transfer the knowledge from DistilBERT to our custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1644ded6-7d6a-43d2-b303-3d43eb316e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6577793-6f3d-48a3-b13e-95116be72685",
   "metadata": {},
   "source": [
    "- Then, let's define the metrics you will be using to evaluate how good is your fine-tuned model (accuracy and f1 score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81da7601-9ca0-45ee-94f8-d9d773fff695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = load_metric(\"accuracy\")\n",
    "   load_f1 = load_metric(\"f1\")\n",
    "  \n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53c65b-3463-4707-a8ad-ebd2de387133",
   "metadata": {},
   "source": [
    "- Define the training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96b56df8-a9ac-41ec-9ad7-86c0e0cec2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"finetuning-sentiment-model-5000-samples\"\n",
    " \n",
    "training_args = TrainingArguments(\n",
    "   output_dir=repo_name,\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=2,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   push_to_hub=False,\n",
    ")\n",
    " \n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaad4946-5446-4cd1-ae2f-8502d2e3037f",
   "metadata": {},
   "source": [
    "- Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e53f771c-d1fb-4a5b-a426-ae5bd53964df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='626' max='626' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [626/626 01:30, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.124400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=626, training_loss=0.11648416519165039, metrics={'train_runtime': 90.4291, 'train_samples_per_second': 110.584, 'train_steps_per_second': 6.923, 'total_flos': 1313372861612160.0, 'train_loss': 0.11648416519165039, 'epoch': 2.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3eee23-b26c-4687-8d27-7d075d76d3a9",
   "metadata": {},
   "source": [
    "- Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a951518-0ee7-4e73-b47b-f679b7e6e628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78692/4164298400.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  load_accuracy = load_metric(\"accuracy\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3359145224094391,\n",
       " 'eval_accuracy': 0.918,\n",
       " 'eval_f1': 0.9168356997971603,\n",
       " 'eval_runtime': 3.2344,\n",
       " 'eval_samples_per_second': 154.589,\n",
       " 'eval_steps_per_second': 9.894,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3db7322-4f03-4fd2-996b-948e3c271da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9975378513336182},\n",
       " {'label': 'LABEL_0', 'score': 0.9927016496658325}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\", model=model,tokenizer=tokenizer, device = device)\n",
    "pipe([\"I love this move\", \"This movie sucks!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581c820-e29d-4a22-96b1-ec2ec2b98b19",
   "metadata": {},
   "source": [
    "## Task 1 Increase the training sample to 10,000 and see how it affetcs the eval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef0f20-f2ef-4fef-b1dd-8e832d770ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = load_dataset(\"imdb\")\n",
    "small_train_dataset = imdb[\"train\"].shuffle(seed=0).select([i for i in list(range(10000))])\n",
    "small_test_dataset = imdb[\"test\"].shuffle(seed=0).select([i for i in list(range(500))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d40ac81-bf5b-4ab7-b713-61fd22151020",
   "metadata": {},
   "source": [
    "## Task 2 Tune the hyperparameters, e.g., learning rate,  and see it affects the eval_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
