{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNKaJz5j_ylj"
   },
   "source": [
    "# 20 newsgroup text classification with BERT finetuning\n",
    "\n",
    "In this notebook, we'll use a pre-trained [BERT](https://arxiv.org/abs/1810.04805) model for text classification using TensorFlow 2 / Keras and HuggingFace's [Transformers](https://github.com/huggingface/transformers). This notebook is based on [\"Predicting Movie Review Sentiment with BERT on TF Hub\"](https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb) by Google and [\"BERT Fine-Tuning Tutorial with PyTorch\"](https://mccormickml.com/2019/07/22/BERT-fine-tuning/) by Chris McCormick.\n",
    "\n",
    "**Note that using a GPU with this notebook is highly recommended.**\n",
    "\n",
    "First, the needed imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ok002ceNB8E7",
    "outputId": "06ef90d2-7518-4209-da66-1dd45c357c78"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import BertTokenizer, BertConfig, TFBertModel\n",
    "from transformers import TFBertForSequenceClassification\n",
    "from transformers import __version__ as transformers_version\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from distutils.version import LooseVersion as LV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import io, sys, os, datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "print('Using TensorFlow version:', tf.__version__,\n",
    "      'Keras version:', tf.keras.__version__,\n",
    "      'Transformers version:', transformers_version)\n",
    "assert(LV(tf.__version__) >= LV(\"2.3.0\"))\n",
    "\n",
    "if len(tf.config.list_physical_devices('GPU')):\n",
    "    from tensorflow.python.client import device_lib\n",
    "    for d in device_lib.list_local_devices():\n",
    "        if d.device_type == 'GPU':\n",
    "            print('GPU', d.physical_device_desc)\n",
    "else:\n",
    "    print('No GPU, using CPU instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 Newsgroups data set\n",
    "\n",
    "Next we'll load the [20 Newsgroups](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html) data set. \n",
    "\n",
    "The dataset contains 20000 messages collected from 20 different Usenet newsgroups (1000 messages from each group):\n",
    "\n",
    "|[]()|[]()|[]()|[]()|\n",
    "| --- | --- |--- | --- |\n",
    "| alt.atheism           | soc.religion.christian   | comp.windows.x     | sci.crypt |               \n",
    "| talk.politics.guns    | comp.sys.ibm.pc.hardware | rec.autos          | sci.electronics |              \n",
    "| talk.politics.mideast | comp.graphics            | rec.motorcycles    | sci.space |                   \n",
    "| talk.politics.misc    | comp.os.ms-windows.misc  | rec.sport.baseball | sci.med |                     \n",
    "| talk.religion.misc    | comp.sys.mac.hardware    | rec.sport.hockey   | misc.forsale |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_DATA_DIR = \"/media/data/20_newsgroup\"\n",
    "\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    i = t.find('\\n\\n')  # skip header\n",
    "                    if 0 < i:\n",
    "                        t = t[i:]\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)\n",
    "\n",
    "labels = np.array(labels)\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a training set and a test set using scikit-learn's `train_test_split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SET = 4000\n",
    "\n",
    "(sentences_train, sentences_test,\n",
    " labels_train, labels_test) = train_test_split(texts, labels, \n",
    "                                               test_size=TEST_SET,\n",
    "                                               shuffle=True, random_state=42)\n",
    "\n",
    "print('Length of training texts:', len(sentences_train))\n",
    "print('Length of training labels:', len(labels_train))\n",
    "print('Length of test texts:', len(sentences_test))\n",
    "print('Length of test labels:', len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The token `[CLS]` is a special token required by BERT at the beginning of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GuE5BqICAne2"
   },
   "outputs": [],
   "source": [
    "sentences_train = [\"[CLS] \" + s for s in sentences_train]\n",
    "sentences_test = [\"[CLS] \" + s for s in sentences_test]\n",
    "\n",
    "print (\"The first training sentence:\")\n",
    "print(sentences_train[0], 'LABEL:', labels_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTREubVNFiz4"
   },
   "source": [
    "Next we specify the pre-trained BERT model we are going to use. The model `\"bert-base-uncased\"` is the lowercased \"base\" model (12-layer, 768-hidden, 12-heads, 110M parameters).\n",
    "\n",
    "We load the used vocabulary from the BERT model, and use the BERT tokenizer to convert the sentences into tokens that match the data the BERT model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "Z474sSC6oe7A",
    "outputId": "fbaa8fd8-bccd-4feb-ce52-beba5d293cfa"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "BERTMODEL='bert-base-uncased'\n",
    "CACHE_DIR='/media/data/transformers-cache/'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERTMODEL,\n",
    "                                          do_lower_case=True,\n",
    "                                          cache_dir=CACHE_DIR)\n",
    "\n",
    "tokenized_train = [tokenizer.tokenize(s) for s in sentences_train]\n",
    "tokenized_test  = [tokenizer.tokenize(s) for s in sentences_test]\n",
    "\n",
    "print (\"The full tokenized first training sentence:\")\n",
    "print (tokenized_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cp9BPRd1tMIo"
   },
   "source": [
    "Now we set the maximum sequence lengths for our training and test sentences as `MAX_LEN_TRAIN` and `MAX_LEN_TEST`. The maximum length supported by the used BERT model is 512.\n",
    "\n",
    "The token `[SEP]` is another special token required by BERT at the end of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cp9BPRd1tMIo"
   },
   "outputs": [],
   "source": [
    "MAX_LEN_TRAIN, MAX_LEN_TEST = 128, 512\n",
    "\n",
    "tokenized_train = [t[:(MAX_LEN_TRAIN-1)]+['SEP'] for t in tokenized_train]\n",
    "tokenized_test  = [t[:(MAX_LEN_TEST-1)]+['SEP'] for t in tokenized_test]\n",
    "\n",
    "print (\"The truncated tokenized first training sentence:\")\n",
    "print (tokenized_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the BERT tokenizer to convert each token into an integer index in the BERT vocabulary. We also pad any shorter sequences to `MAX_LEN_TRAIN` or `MAX_LEN_TEST` indices with trailing zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_train]\n",
    "ids_train = np.array([np.pad(i, (0, MAX_LEN_TRAIN-len(i)), \n",
    "                             mode='constant') for i in ids_train])\n",
    "\n",
    "ids_test = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_test]\n",
    "ids_test = np.array([np.pad(i, (0, MAX_LEN_TEST-len(i)), \n",
    "                            mode='constant') for i in ids_test])\n",
    "\n",
    "print (\"The indices of the first training sentence:\")\n",
    "print (ids_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KhGulL1pExCT"
   },
   "source": [
    "BERT requires *attention masks*, with 1 for each real token in the sequences and 0 for the padding:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDoC24LeEv3N"
   },
   "outputs": [],
   "source": [
    "trainval_masks, test_masks= [], []\n",
    "\n",
    "for seq in ids_train:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  trainval_masks.append(seq_mask)\n",
    "    \n",
    "for seq in ids_test:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  test_masks.append(seq_mask)\n",
    "    \n",
    "trainval_masks = np.array(trainval_masks, dtype=int)\n",
    "test_masks = np.array(test_masks, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFbE-UHvsb7-"
   },
   "source": [
    "We use again scikit-learn's `train_test_split` to use 10% of our training data as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFbE-UHvsb7-"
   },
   "outputs": [],
   "source": [
    "(train_inputs, validation_inputs, \n",
    " train_labels, validation_labels) = train_test_split(ids_train, labels_train, \n",
    "                                                     random_state=42,\n",
    "                                                     test_size=0.1)\n",
    "(train_masks, validation_masks, \n",
    " _, _) = train_test_split(trainval_masks, ids_train,\n",
    "                          random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT also requires *type ids*, which are zero-valued vectors in our case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type_ids = np.zeros(train_masks.shape, dtype=int)\n",
    "validation_type_ids = np.zeros(validation_masks.shape, dtype=int)\n",
    "test_type_ids = np.zeros(test_masks.shape, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gFsCTp_mporB",
    "outputId": "dd067229-1925-4b37-f517-0c14e25420d1"
   },
   "source": [
    "## BERT model initialization\n",
    "\n",
    "We now load a pretrained BERT model with a single linear classification layer added on top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gFsCTp_mporB",
    "outputId": "dd067229-1925-4b37-f517-0c14e25420d1"
   },
   "outputs": [],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(BERTMODEL,\n",
    "                                                        cache_dir=CACHE_DIR,\n",
    "                                                        num_labels=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8o-VEBobKwHk"
   },
   "source": [
    "We use Adam as the optimizer, categorical crossentropy as loss, and then compile the model.\n",
    "\n",
    "`LR` is the learning rate for the Adam optimizer (2e-5 to 5e-5 recommended for BERT finetuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 2e-5\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32, and 2-4 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "EPOCHS = 4\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "history = model.fit([train_inputs, train_masks, train_type_ids], train_labels,\n",
    "                    validation_data=([validation_inputs, validation_masks, validation_type_ids],\n",
    "                                     validation_labels),\n",
    "                    batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-G03mmwH3aI"
   },
   "source": [
    "Let's take a look at loss and accuracy for train and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'], label='training')\n",
    "plt.plot(history.epoch,history.history['val_loss'], label='validation')\n",
    "plt.title('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.xticks(history.epoch, [e+1 for e in history.epoch])\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['accuracy'], label='training')\n",
    "plt.plot(history.epoch,history.history['val_accuracy'], label='validation')\n",
    "plt.title('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.xticks(history.epoch, [e+1 for e in history.epoch])\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkyubuJSOzg3"
   },
   "source": [
    "## Inference\n",
    "\n",
    "For a better measure of the quality of the model, let's see the model accuracy for the test messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_scores = model.evaluate([ids_test, test_masks, test_type_ids],\n",
    "                             labels_test, batch_size=BATCH_SIZE, verbose=2)\n",
    "print(\"Test set %s: %.2f%%\" % (model.metrics_names[1], test_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at classification accuracies separately for each newsgroup, and compute a confusion matrix to see which newsgroups get mixed the most:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict([ids_test, test_masks, test_type_ids])\n",
    "\n",
    "cm=confusion_matrix(labels_test,\n",
    "                    np.argmax(test_predictions[0], axis=1),\n",
    "                    labels=list(range(20)))\n",
    "\n",
    "print('Classification accuracy for each newsgroup:'); print()\n",
    "labels = [l[0] for l in sorted(labels_index.items(), key=lambda x: x[1])]\n",
    "for i,j in enumerate(cm.diagonal()/cm.sum(axis=1)): print(\"%s: %.4f\" % (labels[i].ljust(26), j))\n",
    "print()\n",
    "\n",
    "print('Confusion matrix (rows: true newsgroup; columns: predicted newsgroup):'); print()\n",
    "np.set_printoptions(linewidth=9999)\n",
    "print(cm); print()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cm, cmap=\"gray\", interpolation=\"none\")\n",
    "plt.title('Confusion matrix (rows: true newsgroup; columns: predicted newsgroup)')\n",
    "plt.grid(None)\n",
    "tick_marks = np.arange(len(labels))\n",
    "plt.xticks(tick_marks, labels, rotation=90)\n",
    "plt.yticks(tick_marks, labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT Fine-Tuning Sentence Classification.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "python3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
