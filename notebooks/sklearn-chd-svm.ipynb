{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California housing dataset regression with support vector machines\n",
    "\n",
    "In this notebook, we'll use linear and non-linear [support vector machines (SVMs)](https://scikit-learn.org/stable/modules/svm.html#regression) to estimate median house values on Californian housing districts.\n",
    "\n",
    "First, the needed imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import svm, datasets, __version__\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Then we load the California housing data. First time we need to download the data, which can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chd = datasets.fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll split the data into a training and a test set.\n",
    "\n",
    "Let's also select a single attribute to start the analysis with, say *MedInc*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 5000\n",
    "single_attribute = 'MedInc'\n",
    "\n",
    "X_train_all, X_test_all, y_train, y_test = train_test_split(\n",
    "    chd.data, chd.target, test_size=test_size, shuffle=True)\n",
    "\n",
    "attribute_index = chd.feature_names.index(single_attribute)\n",
    "X_train_single = X_train_all[:, attribute_index].reshape(-1, 1)\n",
    "X_test_single = X_test_all[:, attribute_index].reshape(-1, 1)\n",
    "     \n",
    "print()\n",
    "print('California housing data: train:',len(X_train_all),'test:',len(X_test_all))\n",
    "print()\n",
    "print('X_train_all:', X_train_all.shape)\n",
    "print('X_train_single:', X_train_single.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print()\n",
    "print('X_test_all', X_test_all.shape)\n",
    "print('X_test_single', X_test_single.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data matrix `X_train_all` is a matrix of size (`n_train`, 8), and `X_train_single` contains only the first attribute (*MedInc* by default). The vector `y_train` contains the target value (median house value) for each housing district in the training set.\n",
    "\n",
    "Let's start our analysis with the single attribute. Later, you can set `only_single_attribute = False` to use all eight attributes in the regression.\n",
    "\n",
    "As the final step, let's scale the input data to zero mean and unit variance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_single_attribute = True\n",
    "\n",
    "if only_single_attribute:\n",
    "    X_train = X_train_single\n",
    "    X_test = X_test_single\n",
    "else:\n",
    "    X_train = X_train_all\n",
    "    X_test = X_test_all\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print('X_train: shape:', X_train.shape, 'mean:', X_train.mean(axis=0), 'std:', X_train.std(axis=0))\n",
    "print('X_test: shape:', X_test.shape, 'mean:', X_test.mean(axis=0), 'std:', X_test.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM\n",
    "\n",
    "We begin with SVM using a linear kernel.\n",
    "\n",
    "### Learning\n",
    "\n",
    "Let's use [`LinearSVR`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR), as it is a specialized in linear SVMs. `C` is the penalty parameter.  (The general `SVR` has a similar `kernel=’linear’` option that can also be used.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "C = 1.0\n",
    "lin_reg = svm.LinearSVR(C=C)\n",
    "lin_reg.fit(X_train, y_train)\n",
    "print('coefficients:', lin_reg.coef_)\n",
    "print('intercept:', lin_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the results if we are using only a single attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train.shape[1] == 1:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(X_train, y_train, s=5)\n",
    "    reg_x = np.arange(np.min(X_train), np.max(X_train), 0.01).reshape(-1, 1)\n",
    "    plt.scatter(reg_x, lin_reg.predict(reg_x), s=8, label='linear SVR')\n",
    "    plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "We use *mean squared error* as the performance measure for our regression algorihm: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictions = lin_reg.predict(X_test)\n",
    "print(\"Mean squared error: %.3f\"\n",
    "      % mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear (or kernel) SVM\n",
    "\n",
    "In addition to using a linear kernel, SVMs can be used for non-linear regression by implicitly mapping the input features into high-dimensional feature spaces.  This is sometimes called the *kernel trick*, as the implicit mapping is often computationally cheaper than explicitly operating in the high-dimensional space.\n",
    "\n",
    "### Learning\n",
    "\n",
    "Let's start with a Gaussian kernel or `kernel='rbf'`. \n",
    "\n",
    "A polynomial kernel, that is `kernel='poly'`, is another common choice. The degree of the polynomial is set using the `degree` parameter.\n",
    "\n",
    "Note that non-linear SVMs can be relatively slow to train, so it might be a good idea to start with a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kernel = 'rbf'\n",
    "C = 1.0\n",
    "svm_reg = svm.SVR(kernel=kernel, C=C, gamma='auto')\n",
    "svm_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train.shape[1] == 1:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(X_train, y_train, s=5)\n",
    "    reg_x = np.arange(np.min(X_train), np.max(X_train), 0.01).reshape(-1, 1)\n",
    "    plt.scatter(reg_x, lin_reg.predict(reg_x), s=8, label='linear SVR')\n",
    "    plt.scatter(reg_x, svm_reg.predict(reg_x), s=8, label='non-linear ({}) SVR'.format(kernel))\n",
    "    plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictions = svm_reg.predict(X_test)\n",
    "print(\"Mean squared error: %.3f\"\n",
    "      % mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning\n",
    "\n",
    "Try to reduce the mean squared error of the regression. Experiment with several single attributes and with using all attributes. See the documentation of [LinearSVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR) and [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR) for further options.\n",
    "\n",
    "To further improve the results, it is possible to replace [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), that is scaling the input data to zero mean and unit variance, with more advanced preprocessing.\n",
    "See [Preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-data) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
