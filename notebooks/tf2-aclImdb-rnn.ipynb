{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB movie review sentiment classification with RNNs (aclImdb version)\n",
    "\n",
    "In this notebook, we'll train a recurrent neural network (RNN) for sentiment classification using **Tensorflow** with the **Keras API**. \n",
    "\n",
    "First, the needed imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "print('Using Tensorflow version: {}, and Keras version: {}.'.format(tf.__version__, tf.keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we have GPU available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    from tensorflow.python.client import device_lib\n",
    "    for d in device_lib.list_local_devices():\n",
    "        if d.device_type == 'GPU':\n",
    "            print('GPU', d.physical_device_desc)\n",
    "else:\n",
    "    print('No GPU, using CPU instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB data set\n",
    "\n",
    "Next we'll load the IMDB data set. First time we may have to download the data, which can take a while.\n",
    "\n",
    "The dataset contains 50000 movies reviews from the Internet Movie Database, split into 25000 reviews for training and 25000 reviews for testing. Half of the reviews are positive (1) and half are negative (0).\n",
    "\n",
    "The dataset consists of movie reviews as text files in a directory hierarchy, and we use the `text_dataset_from_directory()` function to create a `tf.data.Dataset()` from the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"/media/gpu-data/imdb\"\n",
    "BATCH_SIZE = 16\n",
    "validation_split=0.2\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train:')\n",
    "imdb_train = tf.keras.utils.text_dataset_from_directory(\n",
    "    DATADIR+\"/aclImdb/train\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=validation_split,\n",
    "    subset='training',\n",
    "    seed=seed\n",
    ")\n",
    "print('\\nValidation:')\n",
    "imdb_valid = tf.keras.utils.text_dataset_from_directory(\n",
    "    DATADIR+\"/aclImdb/train\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=validation_split,\n",
    "    subset='validation',\n",
    "    seed=seed\n",
    ")\n",
    "print('\\nTest:')\n",
    "imdb_test = tf.keras.utils.text_dataset_from_directory(\n",
    "    DATADIR+\"/aclImdb/test\",\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "print('\\nAn example review:')\n",
    "print(imdb_train.unbatch().take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model\n",
    "\n",
    "### Initialization\n",
    "\n",
    "Let's create an RNN model that contains an LSTM layer. The first layer in the network is an *Embedding* layer that converts integer indices to dense vectors of length `embedding_dims`. The output layer contains a single neuron and *sigmoid* non-linearity to match the binary groundtruth (`y_train`). \n",
    "\n",
    "Finally, we `compile()` the model, using *binary crossentropy* as the loss function and [*RMSprop*](https://keras.io/optimizers/#rmsprop) as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = 10000\n",
    "maxlen = 80\n",
    "\n",
    "vectorization_layer = layers.TextVectorization(max_tokens=nb_words,\n",
    "                             output_mode='int',\n",
    "                             output_sequence_length=maxlen)\n",
    "train_text = imdb_train.map(lambda text, labels: text)\n",
    "vectorization_layer.adapt(train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters:\n",
    "embedding_dims = 50\n",
    "lstm_units = 32\n",
    "\n",
    "inputs = keras.Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "x = vectorization_layer(inputs)\n",
    "x = layers.Embedding(input_dim=nb_words, \n",
    "                     output_dim=embedding_dims)(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "x = layers.LSTM(lstm_units)(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs,\n",
    "                    name=\"rnn_model\")\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "Now we are ready to train our model.  An *epoch* means one pass through the whole training data. Note also that we are using a fraction of the training data as our validation set.\n",
    "\n",
    "Note that LSTMs are rather slow to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 5\n",
    "\n",
    "history = model.fit(imdb_train, validation_data=imdb_valid, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data to see how the training progressed. A big gap between training and validation accuracies would suggest overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,3))\n",
    "\n",
    "ax1.plot(history.epoch,history.history['loss'], label='training')\n",
    "ax1.plot(history.epoch,history.history['val_loss'], label='validation')\n",
    "ax1.set_title('loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.legend(loc='best')\n",
    "\n",
    "ax2.plot(history.epoch,history.history['accuracy'], label='training')\n",
    "ax2.plot(history.epoch,history.history['val_accuracy'], label='validation')\n",
    "ax2.set_title('accuracy')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "For a better measure of the quality of the model, let's see the model accuracy for the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(imdb_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the learned model to predict sentiments for new reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myreview = 'This movie was the worst I have ever seen and the actors were horrible.'\n",
    "#myreview = 'This movie is great and I madly love the plot from beginning to end.'\n",
    "\n",
    "p = model.predict([myreview], batch_size=1)\n",
    "print('Predicted sentiment: {}TIVE ({:.4f})'.format(\"POSI\" if p[0,0]>0.5 else \"NEGA\", p[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Two LSTM layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model with two LSTM layers. Optionally, you can also use [Bidirectional layers](https://keras.io/api/layers/recurrent_layers/bidirectional/).\n",
    "\n",
    "The code below is missing the model definition. You can copy any suitable layers from the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# WRITE YOUR CODE HERE\n",
    "\n",
    "ex1_outputs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load tf2-aclImdb-rnn-example-answer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ex1_outputs is not None, \"You need to write the missing model definition\"\n",
    "\n",
    "ex1_model = keras.Model(inputs=ex1_inputs, outputs=ex1_outputs,\n",
    "                    name=\"rnn_model_with_two_lstm_layers\")\n",
    "\n",
    "ex1_model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "print(ex1_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ex1_epochs = 5\n",
    "\n",
    "ex1_history = ex1_model.fit(imdb_train,\n",
    "                            validation_data=imdb_valid,\n",
    "                            epochs=ex1_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data to see how the training progressed. A big gap between training and validation accuracies would suggest overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,3))\n",
    "\n",
    "ax1.plot(ex1_history.epoch,ex1_history.history['loss'], label='training')\n",
    "ax1.plot(ex1_history.epoch,ex1_history.history['val_loss'], label='validation')\n",
    "ax1.set_title('Task 1: loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.legend(loc='best')\n",
    "\n",
    "ax2.plot(ex1_history.epoch,ex1_history.history['accuracy'], label='training')\n",
    "ax2.plot(ex1_history.epoch,ex1_history.history['val_accuracy'], label='validation')\n",
    "ax2.set_title('Task 1: accuracy')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_scores = ex1_model.evaluate(imdb_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (ex1_model.metrics_names[1], ex1_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Model tuning\n",
    "\n",
    "Modify the model further.  Try to improve the classification accuracy on the test set, or experiment with the effects of different parameters.\n",
    "\n",
    "To combat overfitting, you can try for example to add dropout. For [LSTMs](https://keras.io/api/layers/recurrent_layers/lstm/), dropout for inputs and the recurrent states can be set with the `dropout` and `recurrent_dropout` arguments:\n",
    "\n",
    "    x = layers.LSTM(lstm_units, dropout=0.2,\n",
    "                    recurrent_dropout=0.3)(x)\n",
    "\n",
    "Another option is to use early stopping in training. It can be implemented with the [EarlyStopping](https://keras.io/api/callbacks/early_stopping/) callback, for example:\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(\n",
    "                 monitor=\"val_loss\", patience=1,\n",
    "                 restore_best_weights=True)]\n",
    "\n",
    "The callback then needs to be added as an argument to the [`model.fit()`](https://keras.io/api/models/model_training_apis/#fit-method) method.\n",
    "\n",
    "You can also consult the Keras documentation at https://keras.io/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Run this notebook in Google Colaboratory using [this link](https://colab.research.google.com/github/csc-training/intro-to-dl/blob/master/day1/04-tf2-imdb-rnn.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
