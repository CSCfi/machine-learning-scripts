{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Notebook for testing the PyTorch setup\n",
    "\n",
    "This notebook is for testing the [PyTorch](http://pytorch.org/) setup.  Below is a set of required imports.  \n",
    "\n",
    "Run the cell, and no error messages should appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import models, trainers, pre_tokenizers, normalizers, processors\n",
    "\n",
    "from packaging.version import Version as LV\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__)\n",
    "assert(LV(torch.__version__) >= LV(\"2.0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we have GPU available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Using GPU, device name:', torch.cuda.get_device_name(0))\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU found, using CPU instead.') \n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors in PyTorch\n",
    "\n",
    "Tensors are data structures that contain vectors, matrices or higher-dimensional arrays. They are similar to NumPy's ndarrays, except that PyTorch tensors can also run on GPUs and other hardware accelerators. Also check the [PyTorch Tensors tutorial](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html).\n",
    "\n",
    "Let's create some tensors and investigate their shapes and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(3, 4)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x.shape =\",x.shape)\n",
    "print(\"x.dtype =\", x.dtype)\n",
    "print(\"x =\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2, 3],[4, 5, 6]]\n",
    "y = torch.tensor(data, dtype=torch.float)\n",
    "\n",
    "print(\"y.shape =\", y.shape)\n",
    "print(\"y.dtype =\", y.dtype)\n",
    "print(\"y =\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on tensors\n",
    "\n",
    "There are a lot of built-in [operations that can be run on tensors](https://pytorch.org/docs/stable/torch.html). Let's try matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the matrix product y x\n",
    "z = y.matmul(x)\n",
    "\n",
    "print(\"z.shape =\", z.shape)\n",
    "print(\"z.dtype =\", z.dtype)\n",
    "print(\"z =\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Devices\n",
    "\n",
    "We mentioned that PyTorch tensors can also be used on GPUs. We can check what device our tensors is on with `x.device`, we can move it to another device with `x.to(device)` where `device` can be defined dynamically based on if we have GPU available or not. We already did this above with code similar to this:\n",
    "\n",
    "```python\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "```\n",
    "\n",
    "If we don't have a GPU the tensor will just stay on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(before) x.device =\", x.device)\n",
    "x = x.to(device)\n",
    "print(\"(after) x.device =\", x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our tensors are now on the GPU, the matrix multiplication will also take place on the GPU and be much faster (of course not something we would notice in this trivial example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.to(device)\n",
    "z = y.matmul(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"z.device =\", z.device)\n",
    "print(\"z =\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
