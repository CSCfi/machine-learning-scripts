{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB movie review (aclImdb) generation with scaled-down GPT \n",
    "\n",
    "In this notebook, we'll train a scaled-down GPT model from scratch for generating text using **Tensorflow** with the **Keras API**. \n",
    "This notebook is based on Keras code examples [\"Text generation with a miniature GPT\"](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) and [\"GPT text generation from scratch with KerasNLP\"](https://keras.io/examples/generative/text_generation_gpt/).\n",
    "\n",
    "First, the needed imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import keras_nlp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "print('Using Tensorflow version: {}, and Keras version: {}.'.format(tf.__version__, tf.keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we have GPU available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_fp16 = False\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    from tensorflow.python.client import device_lib\n",
    "    for d in device_lib.list_local_devices():\n",
    "        if d.device_type == 'GPU':\n",
    "            print('GPU', d.physical_device_desc)\n",
    "    if use_fp16:\n",
    "        keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "else:\n",
    "    print('No GPU, using CPU instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB data set\n",
    "\n",
    "Next we'll load the entire aclImdb data set. The dataset contains 100,000 movies reviews from the Internet Movie Database.\n",
    "\n",
    "The reviews have been collected into a single text file, and we use the `TextLineDataset()` function to create a `tf.data.Dataset()` from the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"/media/gpu-data/imdb/aclImdb\"\n",
    "DATAFILE = \"aclImdb-nobr-100k.txt\"\n",
    "BATCH_SIZE = 128\n",
    "VOCAB_SIZE = 5000\n",
    "SEQ_LEN = 80  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(DATADIR+\"/\"+DATAFILE)\n",
    "text_ds = text_ds.shuffle(buffer_size=256)\n",
    "text_ds = text_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_ds.unbatch().take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load or create the vocabulary and create the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "VOCABFILE = \"aclImdb-nobr-100k-vocab.txt\"\n",
    "\n",
    "vocabfullpath = DATADIR+\"/\"+VOCABFILE\n",
    "vocab = []\n",
    "if os.path.exists(vocabfullpath):\n",
    "    print('Loading vocabulary from', VOCABFILE)\n",
    "    with open(vocabfullpath, 'r') as fp:\n",
    "        for line in fp:\n",
    "            vocab.append(line[:-1])\n",
    "\n",
    "else:\n",
    "    print('Creating vocabulary')\n",
    "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "        text_ds,\n",
    "        vocabulary_size=VOCAB_SIZE,\n",
    "        lowercase=True,\n",
    "        reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
    "    )\n",
    "    print('Saving vocabulary to', VOCABFILE)\n",
    "    with open(DATADIR+\"/aclImdb-nobr-100k-vocab.txt\", 'w') as fp:\n",
    "        for v in vocab:\n",
    "            fp.write(\"%s\\n\" % v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    lowercase=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packer adds a start token\n",
    "start_packer = keras_nlp.layers.StartEndPacker(\n",
    "    sequence_length=SEQ_LEN,\n",
    "    start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
    ")\n",
    "\n",
    "def preprocess(inputs):\n",
    "    outputs = tokenizer(inputs)\n",
    "    features = start_packer(outputs)\n",
    "    labels = outputs\n",
    "    return features, labels\n",
    "\n",
    "text_ds = text_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT model\n",
    "\n",
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 256\n",
    "FEED_FORWARD_DIM = 256\n",
    "NUM_HEADS = 3\n",
    "NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
    "\n",
    "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")\n",
    "x = embedding_layer(inputs)\n",
    "\n",
    "for _ in range(NUM_LAYERS):\n",
    "    decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
    "        num_heads=NUM_HEADS,\n",
    "        intermediate_dim=FEED_FORWARD_DIM,\n",
    "    )\n",
    "    x = decoder_layer(x)\n",
    "\n",
    "outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "EPOCHS = 6\n",
    "\n",
    "history = model.fit(text_ds, verbose=2, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,3))\n",
    "\n",
    "ax1.plot(history.epoch,history.history['loss'], label='training')\n",
    "ax1.set_title('loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "\n",
    "ax2.plot(history.epoch,history.history['perplexity'], label='training')\n",
    "ax2.set_title('perplexity')\n",
    "ax2.set_xlabel('epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"aclImdb-gpt.h5\"\n",
    "print('Saving model to', fname)\n",
    "model.save(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\n",
    "#review = \"This was a great scary movie which\"\n",
    "#review = \"This was the best movie of\"\n",
    "#review = \"A funny movie\"\n",
    "\n",
    "tokenized_review = np.trim_zeros(tokenizer.tokenize(review).numpy(), 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_review = np.insert(tokenized_review, 0, tokenizer.token_to_id(\"[BOS]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tokens = tf.convert_to_tensor(tokenized_review)\n",
    "prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOKENS_TO_GENERATE = 80\n",
    "\n",
    "#prompt_tokens = tf.convert_to_tensor([tokenizer.token_to_id(\"[BOS]\")])\n",
    "\n",
    "def token_logits_fn(inputs):\n",
    "    cur_len = inputs.shape[1]\n",
    "    output = model(inputs)\n",
    "    return output[:, cur_len - 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tokens = keras_nlp.utils.greedy_search(\n",
    "    token_logits_fn,\n",
    "    prompt_tokens,\n",
    "    max_length=NUM_TOKENS_TO_GENERATE,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Greedy search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "print(\"Random search generated text:\")\n",
    "for i in range(5):\n",
    "    output_tokens = keras_nlp.utils.random_search(\n",
    "        token_logits_fn,\n",
    "        prompt_tokens,\n",
    "        max_length=NUM_TOKENS_TO_GENERATE,\n",
    "        from_logits=True,\n",
    "    )\n",
    "    print(\"{}: {}\".format(i, tokenizer.detokenize(output_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "print(\"Top-P search generated text:\")\n",
    "for i in range(5):\n",
    "    output_tokens = keras_nlp.utils.top_p_search(\n",
    "        token_logits_fn,\n",
    "        prompt_tokens,\n",
    "        max_length=NUM_TOKENS_TO_GENERATE,\n",
    "        p=0.5,\n",
    "        from_logits=True,\n",
    "    )\n",
    "    print(\"{}: {}\".format(i, tokenizer.detokenize(output_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Run this notebook in Google Colaboratory using [this link](https://colab.research.google.com/github/csc-training/intro-to-dl/blob/master/day1/04-tf2-imdb-rnn.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
