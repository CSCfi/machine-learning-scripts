{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB movie review sentiment classification with RNNs\n",
    "\n",
    "In this notebook, we'll train a recurrent neural network (RNN) for sentiment classification using **PyTorch**.\n",
    "\n",
    "First, the needed imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext import datasets\n",
    "import torchtext.transforms as T\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU, device name:', torch.cuda.get_device_name(0))\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU found, using CPU instead.') \n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB data set\n",
    "\n",
    "Next we'll load the IMDB data set, this time using the [TorchText](https://pytorch.org/text/stable/index.html) library.\n",
    "\n",
    "The dataset contains 50000 movies reviews from the Internet Movie Database, split into 25000 reviews for training and 25000 reviews for testing. Half of the reviews are positive and half are negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_project = os.getenv('SLURM_JOB_ACCOUNT')\n",
    "data_dir = os.path.join('/scratch', slurm_project, 'data') if slurm_project else './data'\n",
    "print('data_dir =', data_dir)\n",
    "\n",
    "train_dataset, test_dataset = datasets.IMDB(data_dir, split=('train', 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each data item is a tuple (set of elements) with two parts: a label and the text itself.\n",
    "\n",
    "Let's do a quick count of the labels in the dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(dataset):\n",
    "    counts={}\n",
    "    i=0\n",
    "    for label, text in dataset:\n",
    "        if label not in counts:\n",
    "            counts[label] = 1\n",
    "        else:\n",
    "            counts[label] += 1\n",
    "    for key, value in counts.items():\n",
    "        print(f\"label: {key}, count: {value}\")\n",
    "\n",
    "print('train')\n",
    "count_labels(train_dataset)\n",
    "\n",
    "print('test')\n",
    "count_labels(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything works properly, we should see that we have two labels: `1` and `2`, each with 12500 items per dataset split. Label `1` indicates a negative review, and label `2` a positive one.\n",
    "\n",
    "Let's print the first review in the dataset together with its label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, text in train_dataset:\n",
    "    print(\"First review in the training set:\")\n",
    "    print(text)\n",
    "    print(\"Label:\", label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "Before we start training, we need to process the data into a more suitable format. The text now consists of text strings of variable length, but a neural network typically needs to have fixed-length vectors containing numbers.\n",
    "\n",
    "To achieve this we will:\n",
    "\n",
    "- Create a \"vocabulary\", that is a mapping from each word to a unique number assigned to that word\n",
    "- Convert sentences to lists of integers using the vocabulary\n",
    "- Make fixed-length vectors out of the lists of integers by truncating too long lists, and padding too shorts ones with zeros\n",
    "\n",
    "First, let's make our vocabulary. We'll restrict it to the 10,000 most frequent words. We use the special word `<unk>` (with integer 0) to indicate  any other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of most-frequent words to use\n",
    "nb_words = 10000\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), \n",
    "                                  specials=[\"<unk>\"], max_tokens=nb_words)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `vocab` contains our mapping from word to integer. We can try with some random words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in ['hello', 'the', 'recurrent', '<unk>']:\n",
    "    print(word, \"->\", vocab[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll use PyTorch DataPipes to map the sentences to fixed-length vectors. We'll use a vector length of 80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 80\n",
    "\n",
    "patterns_list = [\n",
    "    (r'\"', '')\n",
    "]\n",
    "\n",
    "patterns_list = [\n",
    "    (r'[,._+ ]+', r' ')\n",
    "]\n",
    "\n",
    "text_transform = T.Sequential(\n",
    "    T.RegexTokenizer(patterns_list),  # Split sentences into words\n",
    "    T.VocabTransform(vocab),          # Transform from words to integers\n",
    "    T.Truncate(maxlen),               # Truncate (shorten) too long lists\n",
    "    T.ToTensor(),                     # Transform to PyTorch tensors\n",
    "    T.PadTransform(maxlen, 0),        # Pad too short lists with zeros\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try the text transformation on a test sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform(\"hello, this is a test sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's apply it to our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the list of transforms to the text\n",
    "# We also switch around so we have the text first and labels second\n",
    "# Finally we subtract 1 from the label so that 0=negative and 1=positive review\n",
    "def apply_transform(x):\n",
    "    return text_transform(x[1]), torch.tensor(x[0]-1, dtype=torch.float)\n",
    "\n",
    "\n",
    "train_dataset_tr = train_dataset.map(apply_transform)\n",
    "test_dataset_tr = test_dataset.map(apply_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create the data loaders with a given batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset_tr, batch_size=batch_size, shuffle=True,\n",
    "                          drop_last=True)\n",
    "test_loader = DataLoader(dataset=test_dataset_tr, batch_size=batch_size, shuffle=False,\n",
    "                         drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model\n",
    "\n",
    "Let's create an RNN model that contains an LSTM layer. The first layer in the network is an *embedding* layer that converts integer indices to dense vectors of length `embedding_dims`. The output layer contains a single neuron and *sigmoid* non-linearity to match the binary groundtruth (0=negative, 1=positive review). \n",
    "\n",
    "All the [neural network building blocks defined in PyTorch can be found in the torch.nn documentation](https://pytorch.org/docs/stable/nn.html).\n",
    "\n",
    "The output of [LSTM in PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) is a 3D tensor of the shape batch_size x sequence_length x lstm_units, that is we get the output after each item in the sequence. Here we only want to have the output after the last item (after the whole sentence has been processed). This means we have to do things a bit more manually and cannot use the simple `nn.Sequential` as in previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters:\n",
    "embedding_dims = 50\n",
    "lstm_units = 32\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(nb_words, embedding_dims)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.lstm = nn.LSTM(embedding_dims, lstm_units, batch_first=True)\n",
    "        self.linear = nn.Linear(lstm_units, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.dropout(x)\n",
    "        x, (hn, cn) = self.lstm(x)    # LSTM also returns the values of the internal h_n and c_n parameters\n",
    "        x = self.linear(x[:, -1, :])  # we pick only the last output after having processed the whole sequence\n",
    "        return self.sigmoid(x.view(-1))\n",
    "\n",
    "model = SimpleRNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Now let's train the RNN model. Note that LSTMs are rather slow to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(output, target):\n",
    "    sentiment_pred = output.round().int()          # set to 0 for <0.5 and 1 for >0.5\n",
    "    correct_ones = sentiment_pred == target.int()  # 1 for correct, 0 for incorrect\n",
    "    return correct_ones.sum().item()               # count number of correct ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    num_batches = 0\n",
    "    num_items = 0\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for data, target in tqdm(data_loader):\n",
    "        # Copy data and targets to GPU\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # Do a forward pass\n",
    "        output = model(data)\n",
    "      \n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Count number of correct digits\n",
    "        total_correct += correct(output, target)\n",
    "        num_items += len(target)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    train_loss = total_loss/num_batches\n",
    "    accuracy = total_correct/num_items\n",
    "    print(f\"Average loss: {train_loss:7f}, accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [binary cross-entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) and [RMSprop optimizer](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Training epoch: {epoch+1}\")\n",
    "    train(train_loader, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Here we have the same `test` function as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    num_batches = 0\n",
    "    num_items = 0\n",
    "\n",
    "    test_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Copy data and targets to GPU\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Do a forward pass\n",
    "            output = model(data)\n",
    "        \n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "            # Count number of correct digits\n",
    "            total_correct += correct(output, target)\n",
    "            num_items += len(target)\n",
    "\n",
    "    test_loss = test_loss/num_batches\n",
    "    accuracy = total_correct/num_items\n",
    "\n",
    "    print(f\"Testset accuracy: {100*accuracy:>0.1f}%, average loss: {test_loss:>7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the learned model to predict sentiments for new reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myreviewtext = 'this movie was the worst i have ever seen and the actors were horrible'\n",
    "myreviewtext = 'this movie was awesome and then best action I have ever seen'\n",
    "\n",
    "input = text_transform(myreviewtext).view(1, -1).to(device)\n",
    "print(input)\n",
    "p = model(input).item()\n",
    "sentiment = \"POSITIVE\" if p > 0.5 else \"NEGATIVE\"\n",
    "print(f'Predicted sentiment: {sentiment} ({p:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Two LSTM layers\n",
    "\n",
    "Create a model with two LSTM layers (hint: there is a `num_layers` option!). Optionally, you can also use bidirectional layers (set `bidirectional=True` in LSTM). See the [LSTM documentation in PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM).\n",
    "\n",
    "You can consult the [PyTorch documentation](https://pytorch.org/docs/stable/index.html), in particular all the [neural network building blocks can be found in the `torch.nn` documentation](https://pytorch.org/docs/stable/nn.html).\n",
    "\n",
    "The code below is missing the model definition. You can copy any suitable layers from the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayeredRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TASK 1: ADD LAYERS HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute cell to see the example answer.\n",
    "\n",
    "**Note:** in Google Colab you have to click and copy the answer manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/pytorch-imdb-rnn-example-answer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_model = TwoLayeredRNN()\n",
    "print(ex1_model)\n",
    "\n",
    "assert len(list(ex1_model.parameters())) > 0, \"ERROR: You need to write the missing model definition above!\"\n",
    "\n",
    "\n",
    "ex1_model = ex1_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_criterion = nn.BCELoss()\n",
    "ex1_optimizer = torch.optim.RMSprop(ex1_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch+1} ...\")\n",
    "    train(train_loader, ex1_model, ex1_criterion, ex1_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader, ex1_model, ex1_criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Model tuning\n",
    "\n",
    "Modify the model further.  Try to improve the classification accuracy on the test set, or experiment with the effects of different parameters.\n",
    "\n",
    "To combat overfitting, you can try for example to add dropout. For [LSTMs](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM), dropout between the LSTM layers can be set with the `dropout` parameter:\n",
    "\n",
    "    self.lstm = nn.LSTM(embedding_dims, lstm_units, num_layers=2,\n",
    "                        batch_first=True, dropout=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to change the batch size, you need to re-define the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
