{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB movie review sentiment classification with RNNs\n",
    "\n",
    "In this notebook, we'll train a recurrent neural network (RNN) for sentiment classification using **PyTorch**.\n",
    "\n",
    "First, the needed imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext import datasets\n",
    "import torchtext.transforms as T\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU, device name:', torch.cuda.get_device_name(0))\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU found, using CPU instead.') \n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB data set\n",
    "\n",
    "Next we'll load the IMDB data set. First time we may have to download the data, which can take a while.\n",
    "\n",
    "The dataset contains 50000 movies reviews from the Internet Movie Database, split into 25000 reviews for training and 25000 reviews for testing. Half of the reviews are positive (1) and half are negative (0).\n",
    "\n",
    "The dataset has already been preprocessed, and each word has been replaced by an integer index.\n",
    "The reviews are thus represented as varying-length sequences of integers.\n",
    "(Word indices begin at \"3\", as \"1\" is used to mark the start of a review and \"2\" represents all out-of-vocabulary words. \"0\" will be used later to pad shorter reviews to a fixed size.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = datasets.IMDB('./data', split=('train', 'test'))\n",
    "#train_dataset, test_dataset = datasets.SST2('./data', split=('train', 'dev'))\n",
    "#train_dataset, test_dataset = datasets.AG_NEWS('./data', split=('train', 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts={}\n",
    "i=0\n",
    "for label, text in train_dataset:\n",
    "    if label not in counts:\n",
    "        counts[label] = 1\n",
    "    else:\n",
    "        counts[label] += 1\n",
    "\n",
    "for key, value in counts.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of most-frequent words to use\n",
    "nb_words = 10000\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), \n",
    "                                  specials=[\"<unk>\"], max_tokens=nb_words)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 80\n",
    "\n",
    "patterns_list = [\n",
    "    (r'\"', '')\n",
    "]\n",
    "\n",
    "text_transform = T.Sequential(\n",
    "    T.RegexTokenizer(patterns_list),\n",
    "    T.VocabTransform(vocab),\n",
    "    T.Truncate(maxlen),\n",
    "    T.ToTensor(),\n",
    "    T.PadTransform(maxlen, 0),\n",
    ")\n",
    "\n",
    "def apply_transform(x):\n",
    "    return text_transform(x[1]), torch.tensor(x[0]-1, dtype=torch.float)\n",
    "\n",
    "\n",
    "train_dataset_tr = train_dataset.map(apply_transform)\n",
    "test_dataset_tr = test_dataset.map(apply_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset_tr, batch_size=batch_size, shuffle=True,\n",
    "                          drop_last=True)\n",
    "test_loader = DataLoader(dataset=test_dataset_tr, batch_size=batch_size, shuffle=False,\n",
    "                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME use this instead? https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model\n",
    "\n",
    "Let's create an RNN model that contains an LSTM layer. The first layer in the network is an *Embedding* layer that converts integer indices to dense vectors of length `embedding_dims`. The output layer contains a single neuron and *sigmoid* non-linearity to match the binary groundtruth (`y_train`). \n",
    "\n",
    "All the [neural network building blocks defined in PyTorch can be found in the torch.nn documentation](https://pytorch.org/docs/stable/nn.html).\n",
    "\n",
    "The output of the last layer should be normalized with softmax, but this is actually included implicitly in the loss function in PyTorch (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters:\n",
    "embedding_dims = 50\n",
    "lstm_units = 32\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(nb_words, embedding_dims)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.lstm = nn.LSTM(embedding_dims, lstm_units, batch_first=True)\n",
    "        self.linear = nn.Linear(lstm_units, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.dropout(x)\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "        x = self.linear(x[:, -1, :])\n",
    "        return self.sigmoid(x.view(-1))\n",
    "\n",
    "model = SimpleRNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Now let's train the RNN model. Note that LSTMs are rather slow to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(output, target):\n",
    "    sentiment_pred = output.round().int()          # set to 0 for <0.5 and 1 for >0.5\n",
    "    correct_ones = sentiment_pred == target.int()  # 1 for correct, 0 for incorrect\n",
    "    return correct_ones.sum().item()               # count number of correct ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    num_batches = 0\n",
    "    num_items = 0\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for data, target in tqdm(data_loader):\n",
    "        # Copy data and targets to GPU\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # Do a forward pass\n",
    "        output = model(data)\n",
    "      \n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss\n",
    "        num_batches += 1\n",
    "        \n",
    "        #print(output)\n",
    "        #print(target)\n",
    "\n",
    "        # Count number of correct digits\n",
    "        total_correct += correct(output, target)\n",
    "        num_items += len(target)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    train_loss = total_loss/num_batches\n",
    "    accuracy = total_correct/num_items\n",
    "    print(f\"Average loss: {train_loss:7f}, accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Training epoch: {epoch+1}\")\n",
    "    train(train_loader, model, criterion, optimizer)\n",
    "    #test(test_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Here we have the same `test` function as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    num_batches = 0\n",
    "    num_items = 0\n",
    "\n",
    "    test_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Copy data and targets to GPU\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Do a forward pass\n",
    "            output = model(data)\n",
    "        \n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "            # Count number of correct digits\n",
    "            total_correct += correct(output, target)\n",
    "            num_items += len(target)\n",
    "\n",
    "    test_loss = test_loss/num_batches\n",
    "    accuracy = total_correct/num_items\n",
    "\n",
    "    print(f\"Testset accuracy: {100*accuracy:>0.1f}%, average loss: {test_loss:>7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myreviewtext = 'this movie was the worst i have ever seen and the actors were horrible'\n",
    "#myreviewtext = 'this movie was awesome and then best action I have ever seen'\n",
    "\n",
    "input = text_transform(myreviewtext).view(1, -1).to(device)\n",
    "print(input)\n",
    "output = model(input)\n",
    "print(output.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Two LSTM layers\n",
    "\n",
    "Create a model with two LSTM layers. Optionally, you can also use bidirectional layers (set `bidirectional=False` in LSTM. See the [LSTM documentation in PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM).\n",
    "\n",
    "You can consult the [PyTorch documentation](https://pytorch.org/docs/stable/index.html), in particular all the [neural network building blocks can be found in the `torch.nn` documentation](https://pytorch.org/docs/stable/nn.html).\n",
    "\n",
    "The code below is missing the model definition. You can copy any suitable layers from the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayeredRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TASK 1: ADD LAYERS HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute cell to see the example answer.\n",
    "\n",
    "**Note:** in Google Colab you have to click and copy the answer manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/pytorch-mnist-rnn-example-answer.py\n",
    "embedding_dims = 50\n",
    "lstm_units = 32\n",
    "\n",
    "class TwoLayeredRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(nb_words, embedding_dims)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.lstm = nn.LSTM(embedding_dims, lstm_units, num_layers=2,\n",
    "                            batch_first=True)\n",
    "        self.linear = nn.Linear(lstm_units, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.dropout(x)\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "        x = self.linear(x[:, -1, :])\n",
    "        return self.sigmoid(x.view(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_model = TwoLayeredRNN()\n",
    "print(ex1_model)\n",
    "\n",
    "assert len(list(ex1_model.parameters())) > 0, \"ERROR: You need to write the missing model definition above!\"\n",
    "\n",
    "\n",
    "ex1_model = ex1_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_criterion = nn.BCELoss()\n",
    "ex1_optimizer = torch.optim.RMSprop(ex1_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch+1} ...\")\n",
    "    train(train_loader, ex1_model, ex1_criterion, ex1_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader, ex1_model, ex1_criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Model tuning\n",
    "\n",
    "Modify the model further.  Try to improve the classification accuracy on the test set, or experiment with the effects of different parameters.\n",
    "\n",
    "To combat overfitting, you can try for example to add dropout. For [LSTMs](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM), dropout between the LSTM layers can be set with the `dropout` parameter:\n",
    "\n",
    "    self.lstm = nn.LSTM(embedding_dims, lstm_units, num_layers=2,\n",
    "                        batch_first=True, dropout=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to change the batch size, you need to re-define the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Run this notebook in Google Colaboratory using [this link](https://colab.research.google.com/github/csc-training/intro-to-dl/blob/master/day1/optional/pytorch-mnist-mlp.ipynb).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
