{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST handwritten digits dimensionality reduction with scikit-learn\n",
    "\n",
    "In this notebook, we'll use some popular methods to reduce the dimensionality of MNIST digits data before classification.  \n",
    "\n",
    "First, the needed imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, __version__\n",
    "from sklearn import decomposition, feature_selection\n",
    "from skimage.measure import block_reduce\n",
    "from skimage.feature import canny\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from distutils.version import LooseVersion as LV\n",
    "assert(LV(__version__) >= LV(\"0.20\")), \"Version >= 0.20 of sklearn is required.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the MNIST data. First time it may download the data, which can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist = datasets.fetch_openml('mnist_784')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    mnist['data'], mnist['target'], test_size=10000, shuffle=True)\n",
    "     \n",
    "print('MNIST data loaded: train:',len(X_train),'test:',len(X_test))\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test', X_test.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature extraction\n",
    "\n",
    "### 1.1 PCA\n",
    "\n",
    "[Principal component analysis](http://scikit-learn.org/stable/modules/decomposition.html#pca) (PCA) is a standard method to decompose a high-dimensional dataset in a set of successive orthogonal components that explain a maximum amount of the variance. Here we project the data into `n_components` principal components. The components have the maximal possible variance under the orthogonality constraint.\n",
    "\n",
    "The option `whiten=True` can be used to whiten the outputs to have unit component-wise variances.  Its usefulness depends on the model to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_components = 50\n",
    "pca = decomposition.PCA(n_components=n_components, whiten=True)\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "print('X_pca:', X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the amount of variance explained by the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(n_components)+1, pca.explained_variance_)\n",
    "plt.title('Explained variance by PCA components')\n",
    "plt.ylabel('explained variance')\n",
    "plt.xlabel('PCA component');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Image feature extraction\n",
    "\n",
    "There are a lot of different feature extraction methods for image data.  Common ones include extraction of colors, textures, and shapes from images, or detection of edges, corners, lines, blobs, or templates.  Let's try a simple filtering-based method to reduce the dimensionality of the features, and a widely-used edge detector.\n",
    "\n",
    "The [`measure.block_reduce()`](http://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.block_reduce) function from scikit-image applies a function (for_example `np.mean`, `np.max` or `np.median`) to blocks of the image, resulting in a downsampled image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_img = X_train.reshape(-1, 28, 28)\n",
    "filter_size = 2\n",
    "X_train_img_downsampled = block_reduce(X_train_img, \n",
    "                                       block_size=(1, filter_size, filter_size), \n",
    "                                       func=np.mean)\n",
    "\n",
    "print('X_train_img:', X_train_img.shape)\n",
    "print('X_train_img_downsampled:', X_train_img_downsampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`feature.canny()`](http://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.canny) function applies the [Canny edge detector](https://en.wikipedia.org/wiki/Canny_edge_detector) to extract edges from the image.  Processing all images may take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sigma = 1.0\n",
    "X_train_img_canny = np.zeros(X_train_img.shape)\n",
    "for i in range(X_train_img.shape[0]):\n",
    "    X_train_img_canny[i,:,:] = canny(X_train_img[i,:,:], sigma=sigma)\n",
    "print('X_train_img_canny:', X_train_img_canny.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the original and filtered digit images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pltsize=1\n",
    "\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "plt.suptitle('Original')\n",
    "plt.subplots_adjust(top=0.8)\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train_img[i,:,:], cmap=\"gray\", interpolation='none')\n",
    "\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "plt.suptitle('Downsampled with a %dx%d filter' % (filter_size, filter_size))\n",
    "plt.subplots_adjust(top=0.8)\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train_img_downsampled[i,:,:], cmap=\"gray\", interpolation='none')\n",
    "    \n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "plt.suptitle('Canny edge detection with sigma=%.2f' % sigma)\n",
    "plt.subplots_adjust(top=0.8)\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train_img_canny[i,:,:], cmap=\"gray\", interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature selection\n",
    "\n",
    "### 2.1 Low variance\n",
    "\n",
    "The MNIST digits have a lot of components (pixels) with little variance.  These components are not particularly useful for discriminating between the classes, so they can probably be removed safely.  Let's first draw the component-wise variances of MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variances = np.var(X_train, axis=0)\n",
    "plt.figure()\n",
    "plt.plot(variances)\n",
    "plt.title('Component-wise variance of MNIST digits')\n",
    "plt.ylabel('variance')\n",
    "plt.xlabel('component');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variances can also be plotted for each pixel in the image plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(variances.reshape(28,28), cmap=sns.color_palette(\"Blues\"))\n",
    "plt.title('Pixel-wise variance of MNIST digits')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select an appropriate `variance_threshold` based on the *\"Component-wise variance of MNIST digits\"* figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "variance_threshold = 1000\n",
    "lv = feature_selection.VarianceThreshold(threshold=variance_threshold)\n",
    "X_lv = lv.fit_transform(X_train)\n",
    "print('X_lv:', X_lv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Univariate feature selection\n",
    "\n",
    "Another method for feature selection is to select the *k* best features based on univariate statistical tests between the features and the class of each sample.  Therefore, this is a supervised method and we need to include `y_train` in `fit_transform()`.\n",
    "See [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) for the set of available statistical tests and other further options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "k = 50\n",
    "ukb = feature_selection.SelectKBest(k=k)\n",
    "X_ukb = ukb.fit_transform(X_train, y_train)\n",
    "print('X_ukb:', X_ukb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check which features (that is, pixels in case) got selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "support = ukb.get_support()\n",
    "plt.figure()\n",
    "sns.heatmap(support.reshape(28,28), cmap=sns.color_palette(\"Blues\"))\n",
    "#with sns.axes_style(\"white\"):\n",
    "#    plt.imshow(support.reshape(28,28), interpolation='none')\n",
    "plt.title('Support of SelectKBest() with k=%d' % k)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification with dimension-reduced data \n",
    "\n",
    "Test nearest neighbor classifiers and/or decision trees with the lower-dimensional data.  Compare to classification using the original input data.\n",
    "\n",
    "Note that you need to transform the test data into the lower-dimensional space using `transform()`.  Here is an example for PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_pca = pca.transform(X_test)\n",
    "print('X_test_pca:', X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Other methods for dimensionality reduction\n",
    "\n",
    "Study and experiment with additional dimensionality reduction methods based on [decomposing](http://scikit-learn.org/stable/modules/decomposition.html) or [feature selection](http://scikit-learn.org/stable/modules/feature_selection.html).  See also [unsupervised dimensionality reduction](http://scikit-learn.org/stable/modules/unsupervised_reduction.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
